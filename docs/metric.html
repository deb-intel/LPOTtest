

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Metrics &mdash; Intel® Low Precision Optimization Tool  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tutorial" href="tutorial.html" />
    <link rel="prev" title="Dataset" href="dataset.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Intel® Low Precision Optimization Tool
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../welcome.html">Introduction to Intel LPOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-documentation/api-introduction.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples_readme.html">Examples</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="doclist.html">Developer Documentation</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="doclist.html#getting-started">Getting Started</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../api-documentation/api-introduction.html">API Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="transform.html">Transform</a></li>
<li class="toctree-l3"><a class="reference internal" href="dataset.html">Dataset</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Metrics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#how-to-use-metrics">How to use Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#built-in-metric-support-list">Built-in metric support list</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="tutorial.html">Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples_readme.html">Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="ux.html">LPOT UX</a></li>
<li class="toctree-l3"><a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html">Intel oneAPI AI Analytics Toolkit Get Started Guide</a></li>
<li class="toctree-l3"><a class="reference external" href="https://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics">AI and Analytics Samples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="doclist.html#deep-dive">Deep Dive</a></li>
<li class="toctree-l2"><a class="reference internal" href="doclist.html#advanced-topics">Advanced Topics</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../releases_info.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../security_policy.html">Security Policy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel® Low Precision Optimization Tool</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="doclist.html">Developer Documentation</a> &raquo;</li>
        
      <li>Metrics</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/docs/metric.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="metrics">
<h1>Metrics<a class="headerlink" href="#metrics" title="Permalink to this headline">¶</a></h1>
<p>In terms of evaluating the performance of a specific model, we should have general metrics to measure the performance of different models. Different frameworks always have their own Metric module but with different features and APIs. LPOT Metrics supports code-free configuration through a yaml file, with built-in metrics, so that LPOT can achieve performance and accuracy without code changes from the user. In special cases, users can also register their own metric classes through the LPOT method.</p>
<div class="section" id="how-to-use-metrics">
<h2>How to use Metrics<a class="headerlink" href="#how-to-use-metrics" title="Permalink to this headline">¶</a></h2>
<div class="section" id="config-built-in-metric-in-a-yaml-file">
<h3>Config built-in metric in a yaml file<a class="headerlink" href="#config-built-in-metric-in-a-yaml-file" title="Permalink to this headline">¶</a></h3>
<p>Users can specify an LPOT built-in metric such as shown below:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">evaluation</span><span class="p">:</span>
  <span class="nt">accuracy</span><span class="p">:</span>
    <span class="nt">metric</span><span class="p">:</span>
      <span class="nt">topk</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
</pre></div>
</div>
</div>
<div class="section" id="config-custom-metric-in-code">
<h3>Config custom metric in code<a class="headerlink" href="#config-custom-metric-in-code" title="Permalink to this headline">¶</a></h3>
<p>Users can also register their own metric as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Metric</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># init code here</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="c1"># add preds and labels to storage</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># clear preds and labels storage</span>

    <span class="k">def</span> <span class="nf">result</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># calculate accuracy</span>
        <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
</div>
<p>The result() function returns a higher-is-better scalar to reflect model accuracy on an evaluation dataset.</p>
<p>After defining the metric class, users need to register it with a user-defined metric name and the metric class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">from</span> <span class="nn">lpot.quantization</span> <span class="kn">import</span> <span class="n">Quantization</span><span class="p">,</span> <span class="n">common</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">Metric</span><span class="p">(</span><span class="n">NewMetric</span><span class="p">,</span> <span class="s1">&#39;metric_name&#39;</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">calib_dataloader</span> <span class="o">=</span> <span class="n">dataloader</span>
<span class="n">q_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="built-in-metric-support-list">
<h2>Built-in metric support list<a class="headerlink" href="#built-in-metric-support-list" title="Permalink to this headline">¶</a></h2>
<p>LPOT supports some built-in metrics that are popularly used in industry.</p>
<p>Refer to <a class="reference external" href="https://github.com/intel/lpot/tree/master/examples/helloworld/tf_example1">this HelloWorld example</a> on how to config a built-in metric.</p>
<div class="section" id="tensorflow">
<h3>TensorFlow<a class="headerlink" href="#tensorflow" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Parameters</th>
<th align="left">Inputs</th>
<th align="left">Comments</th>
<th align="left">Usage(In yaml file)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">topk(k)</td>
<td align="left">k (int, default=1): Number of top elements to look at for computing accuracy</td>
<td align="left">preds, labels</td>
<td align="left">Computes top k predictions accuracy.</td>
<td align="left">metric: <br> &ensp;&ensp; topk: <br> &ensp;&ensp;&ensp;&ensp; k: 1</td>
</tr>
<tr>
<td align="left">Accuracy()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes accuracy classification score.</td>
<td align="left">metric: <br> &ensp;&ensp; Accuracy: {}</td>
</tr>
<tr>
<td align="left">Loss()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">A dummy metric for directly printing loss, it calculates the average of predictions. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/_modules/mxnet/metric.html#Loss">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; Loss: {}</td>
</tr>
<tr>
<td align="left">MAE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Absolute Error (MAE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; MAE: {}</td>
</tr>
<tr>
<td align="left">RMSE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes Root Mean Squred Error (RMSE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; RMSE: {}</td>
</tr>
<tr>
<td align="left">MSE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Squared Error (MSE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; MSE: {}</td>
</tr>
<tr>
<td align="left">F1()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes the F1 score of a binary classification problem.</td>
<td align="left">metric: <br> &ensp;&ensp; F1: {}</td>
</tr>
<tr>
<td align="left">COCOmAP(anno_path)</td>
<td align="left">anno_path(str, default=None):annotation path</td>
<td align="left">preds, labels</td>
<td align="left">preds is a tuple which supports 2 length: 3 and 4. <br> If its length is 3, it should contain boxes, scores, classes in turn. <br> If its length is 4, it should contain target_boxes_num, boxes, scores, classes in turn <br> labels is a tuple which contains bbox, str_label, int_label, image_id inturn <br> the length of one of str_label and int_label can be 0</td>
<td align="left">metric: <br> &ensp;&ensp; COCOmAP: <br> &ensp;&ensp;&ensp;&ensp; anno_path: /path/to/annotation <br> If anno_path is not set, metric will use built-in coco_label_map</td>
</tr>
<tr>
<td align="left">BLEU()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">BLEU score computation between labels and predictions. An approximate BLEU scoring method since we do not glue word pieces or decode the ids and tokenize the output. By default, we use ngram order of 4 and use brevity penalty. Also, this does not have beam search</td>
<td align="left">metric: <br> &ensp;&ensp; BLEU: {}</td>
</tr>
<tr>
<td align="left">SquadF1()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Evaluate v1.1 of the SQuAD dataset</td>
<td align="left">metric: <br> &ensp;&ensp; SquadF1: {}</td>
</tr>
</tbody>
</table></div>
<div class="section" id="pytorch">
<h3>PyTorch<a class="headerlink" href="#pytorch" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Parameters</th>
<th align="left">Inputs</th>
<th align="left">Comments</th>
<th align="left">Usage(In yaml file)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">topk(k)</td>
<td align="left">k (int, default=1): Number of top elements to look at for computing accuracy</td>
<td align="left">preds, labels</td>
<td align="left">Calculates the top-k categorical accuracy.</td>
<td align="left">metric: <br> &ensp;&ensp; topk: <br> &ensp;&ensp;&ensp;&ensp; k: 1</td>
</tr>
<tr>
<td align="left">Accuracy()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Calculates the accuracy for binary, multiclass and multilabel data. <br> Please refer <a href="https://pytorch.org/ignite/metrics.html#ignite.metrics.Accuracy">Pytorch docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; Accuracy: {}</td>
</tr>
<tr>
<td align="left">Loss()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">A dummy metric for directly printing loss, it calculates the average of predictions. <br> Please refer <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/_modules/mxnet/metric.html#Loss">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; Loss: {}</td>
</tr>
<tr>
<td align="left">MAE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Calculates the mean absolute error. <br> Please refer <a href="https://pytorch.org/ignite/metrics.html#ignite.metrics.MeanAbsoluteError">Pytorch docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; MAE: {}</td>
</tr>
<tr>
<td align="left">RMSE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Calculates the root mean squared error. <br> Please refer <a href="https://pytorch.org/ignite/metrics.html#ignite.metrics.RootMeanSquaredError">Pytorch docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; RMSE: {}</td>
</tr>
<tr>
<td align="left">MSE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Calculates the mean squared error. <br> Please refer <a href="https://pytorch.org/ignite/metrics.html#ignite.metrics.MeanSquaredError">Pytorch docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; MSE: {}</td>
</tr>
<tr>
<td align="left">F1()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes the F1 score of a binary classification problem.</td>
<td align="left">metric: <br> &ensp;&ensp; F1: {}</td>
</tr>
</tbody>
</table></div>
<div class="section" id="mxnet">
<h3>MXNet<a class="headerlink" href="#mxnet" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Parameters</th>
<th align="left">Inputs</th>
<th align="left">Comments</th>
<th align="left">Usage(In yaml file)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">topk(k)</td>
<td align="left">k (int, default=1): Number of top elements to look at for computing accuracy</td>
<td align="left">preds, labels</td>
<td align="left">Computes top k predictions accuracy.</td>
<td align="left">metric: <br> &ensp;&ensp; topk: <br> &ensp;&ensp;&ensp;&ensp; k: 1</td>
</tr>
<tr>
<td align="left">Accuracy()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes accuracy classification score. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.Accuracy">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; Accuracy: {}</td>
</tr>
<tr>
<td align="left">Loss()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">A dummy metric for directly printing loss, it calculates the average of predictions. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/_modules/mxnet/metric.html#Loss">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; Loss: {}</td>
</tr>
<tr>
<td align="left">MAE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Absolute Error (MAE) loss. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.MAE">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; MAE: {}</td>
</tr>
<tr>
<td align="left">RMSE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes Root Mean Squred Error (RMSE) loss. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.RMSE">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; RMSE: {}</td>
</tr>
<tr>
<td align="left">MSE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Squared Error (MSE) loss. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.MSE">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; MSE: {}</td>
</tr>
<tr>
<td align="left">F1()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes the F1 score of a binary classification problem. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/metric/index.html#mxnet.metric.F1">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; F1: {}</td>
</tr>
</tbody>
</table></div>
<div class="section" id="onnxrt">
<h3>ONNXRT<a class="headerlink" href="#onnxrt" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">Parameters</th>
<th align="left">Inputs</th>
<th align="left">Comments</th>
<th align="left">Usage(In yaml file)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">topk(k)</td>
<td align="left">k (int, default=1): Number of top elements to look at for computing accuracy</td>
<td align="left">preds, labels</td>
<td align="left">Computes top k predictions accuracy.</td>
<td align="left">metric: <br> &ensp;&ensp; topk: <br> &ensp;&ensp;&ensp;&ensp; k: 1</td>
</tr>
<tr>
<td align="left">Accuracy()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes accuracy classification score.</td>
<td align="left">metric: <br> &ensp;&ensp; Accuracy: {}</td>
</tr>
<tr>
<td align="left">Loss()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">A dummy metric for directly printing loss, it calculates the average of predictions. <br> Please refer to <a href="https://mxnet.apache.org/versions/1.7.0/api/python/docs/_modules/mxnet/metric.html#Loss">MXNet docs</a> for details.</td>
<td align="left">metric: <br> &ensp;&ensp; Loss: {}</td>
</tr>
<tr>
<td align="left">MAE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Absolute Error (MAE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; MAE: {}</td>
</tr>
<tr>
<td align="left">RMSE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes Root Mean Squred Error (RMSE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; RMSE: {}</td>
</tr>
<tr>
<td align="left">MSE()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes Mean Squared Error (MSE) loss.</td>
<td align="left">metric: <br> &ensp;&ensp; MSE: {}</td>
</tr>
<tr>
<td align="left">F1()</td>
<td align="left">None</td>
<td align="left">preds, labels</td>
<td align="left">Computes the F1 score of a binary classification problem.</td>
<td align="left">metric: <br> &ensp;&ensp; F1: {}</td>
</tr>
</tbody>
</table></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="tutorial.html" class="btn btn-neutral float-right" title="Tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="dataset.html" class="btn btn-neutral float-left" title="Dataset" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Intel® LPOT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>