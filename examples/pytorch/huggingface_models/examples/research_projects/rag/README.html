

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Intro &mdash; Intel® Low Precision Optimization Tool  documentation</title>
  

  
  <link rel="stylesheet" href="../../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../../" src="../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../../../index.html" class="icon icon-home"> Intel® Low Precision Optimization Tool
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../welcome.html">Introduction to Intel LPOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../docs/introduction.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../docs/doclist.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../releases-info.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../CONTRIBUTING.html">Contributing Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../SECURITY.html">Security Policy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">Intel® Low Precision Optimization Tool</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Intro</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../../../_sources/examples/pytorch/huggingface_models/examples/research_projects/rag/README.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="intro">
<h1>Intro<a class="headerlink" href="#intro" title="Permalink to this headline">¶</a></h1>
<p>Authors: &#64;patrickvonplaten and &#64;lhoestq</p>
<p>Aimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn’t be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time.</p>
<p>A RAG model encapsulates two core components: a question encoder and a generator.
During a forward pass, we encode the input with the question encoder and pass it
to the retriever to extract relevant context documents. The documents are then prepended to the input.
Such contextualized inputs are passed to the generator.</p>
<p>Read more about RAG  at https://arxiv.org/abs/2005.11401.</p>
</div>
<div class="section" id="finetuning">
<h1>Finetuning<a class="headerlink" href="#finetuning" title="Permalink to this headline">¶</a></h1>
<p>Our finetuning logic is based on scripts from <a class="reference external" href="https://github.com/huggingface/transformers/tree/master/examples/seq2seq"><code class="docutils literal notranslate"><span class="pre">examples/seq2seq</span></code></a>. We accept training data in the same format as specified there - we expect a directory consisting of 6 text files:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>train.source
train.target
val.source
val.target
test.source
test.target
</pre></div>
</div>
<p>A sample finetuning command (run <code class="docutils literal notranslate"> <span class="pre">./examples/research_projects/rag/finetune_rag.py</span> <span class="pre">--help</span></code> to list all available options):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python examples/research_projects/rag/finetune_rag.py <span class="se">\</span>
    --data_dir <span class="nv">$DATA_DIR</span> <span class="se">\</span>
    --output_dir <span class="nv">$OUTPUT_DIR</span> <span class="se">\</span>
    --model_name_or_path <span class="nv">$MODEL_NAME_OR_PATH</span> <span class="se">\</span>
    --model_type rag_sequence <span class="se">\</span>
    --fp16 <span class="se">\</span>
    --gpus <span class="m">8</span>
</pre></div>
</div>
<p>We publish two <code class="docutils literal notranslate"><span class="pre">base</span></code> models which can serve as a starting point for finetuning on downstream tasks (use them as <code class="docutils literal notranslate"><span class="pre">model_name_or_path</span></code>):</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/facebook/rag-sequence-base"><code class="docutils literal notranslate"><span class="pre">facebook/rag-sequence-base</span></code></a> - a base for finetuning <code class="docutils literal notranslate"><span class="pre">RagSequenceForGeneration</span></code> models,</p></li>
<li><p><a class="reference external" href="https://huggingface.co/facebook/rag-token-base"><code class="docutils literal notranslate"><span class="pre">facebook/rag-token-base</span></code></a> - a base for finetuning <code class="docutils literal notranslate"><span class="pre">RagTokenForGeneration</span></code> models.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">base</span></code> models initialize the question encoder with <a class="reference external" href="https://huggingface.co/facebook/dpr-question_encoder-single-nq-base"><code class="docutils literal notranslate"><span class="pre">facebook/dpr-question_encoder-single-nq-base</span></code></a> and the generator with <a class="reference external" href="https://huggingface.co/facebook/bart-large"><code class="docutils literal notranslate"><span class="pre">facebook/bart-large</span></code></a>.</p>
<p>If you would like to initialize finetuning with a base model using different question encoder and generator architectures, you can build it with a consolidation script, e.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">examples</span><span class="o">/</span><span class="n">research_projects</span><span class="o">/</span><span class="n">rag</span><span class="o">/</span><span class="n">consolidate_rag_checkpoint</span><span class="o">.</span><span class="n">py</span> \
    <span class="o">--</span><span class="n">model_type</span> <span class="n">rag_sequence</span> \
    <span class="o">--</span><span class="n">generator_name_or_path</span> <span class="n">facebook</span><span class="o">/</span><span class="n">bart</span><span class="o">-</span><span class="n">large</span><span class="o">-</span><span class="n">cnn</span> \
    <span class="o">--</span><span class="n">question_encoder_name_or_path</span> <span class="n">facebook</span><span class="o">/</span><span class="n">dpr</span><span class="o">-</span><span class="n">question_encoder</span><span class="o">-</span><span class="n">single</span><span class="o">-</span><span class="n">nq</span><span class="o">-</span><span class="n">base</span> \
    <span class="o">--</span><span class="n">dest</span> <span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">checkpoint</span>
</pre></div>
</div>
<p>You will then be able to pass <code class="docutils literal notranslate"><span class="pre">path/to/checkpoint</span></code> as <code class="docutils literal notranslate"><span class="pre">model_name_or_path</span></code> to the <code class="docutils literal notranslate"><span class="pre">finetune_rag.py</span></code> script.</p>
<div class="section" id="document-retrieval">
<h2>Document Retrieval<a class="headerlink" href="#document-retrieval" title="Permalink to this headline">¶</a></h2>
<p>When running distributed fine-tuning, each training worker needs to retrieve contextual documents
for its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,
one with <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html"><code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code></a> communication package and the other
with <a class="reference external" href="https://docs.ray.io/en/master/"><code class="docutils literal notranslate"><span class="pre">Ray</span></code></a>.</p>
<p>This option can be configured with the <code class="docutils literal notranslate"><span class="pre">--distributed_retriever</span></code> flag which can either be set to <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> or <code class="docutils literal notranslate"><span class="pre">ray</span></code>.
By default this flag is set to <code class="docutils literal notranslate"><span class="pre">pytorch</span></code>.</p>
<p>For the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used
to collect the inputs from the other training workers and send back the corresponding document embeddings.</p>
<p>For the Ray implementation, the index is loaded in <em>separate</em> process(es). The training workers randomly select which
retriever worker to query. To use Ray for distributed retrieval, you have to set the <code class="docutils literal notranslate"><span class="pre">--distributed_retriever</span></code> arg to <code class="docutils literal notranslate"><span class="pre">ray</span></code>.
To configure the number of retrieval workers (the number of processes that load the index), you can set the <code class="docutils literal notranslate"><span class="pre">num_retrieval_workers</span></code> flag.
Also make sure to start the Ray cluster before running fine-tuning.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start a single-node Ray cluster.</span>
ray start --head

python examples/research_projects/rag/finetune_rag.py <span class="se">\</span>
    --data_dir <span class="nv">$DATA_DIR</span> <span class="se">\</span>
    --output_dir <span class="nv">$OUTPUT_DIR</span> <span class="se">\</span>
    --model_name_or_path <span class="nv">$MODEL_NAME_OR_PATH</span> <span class="se">\</span>
    --model_type rag_sequence <span class="se">\</span>
    --fp16 <span class="se">\</span>
    --gpus <span class="m">8</span>
    --distributed_retriever ray <span class="se">\</span>
    --num_retrieval_workers <span class="m">4</span>

<span class="c1"># Stop the ray cluster once fine-tuning has finished.</span>
ray stop
</pre></div>
</div>
<p>Using Ray can lead to retrieval speedups on multi-GPU settings since multiple processes load the index rather than
just the rank 0 training worker. Using Ray also allows you to load the index on GPU since the index is loaded on a separate
processes than the model, while with pytorch distributed retrieval, both are loaded in the same process potentially leading to GPU OOM.</p>
</div>
</div>
<div class="section" id="evaluation">
<h1>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h1>
<p>Our evaluation script enables two modes of evaluation (controlled by the <code class="docutils literal notranslate"><span class="pre">eval_mode</span></code> argument): <code class="docutils literal notranslate"><span class="pre">e2e</span></code> - end2end evaluation, returns EM (exact match) and F1 scores calculated for the downstream task and <code class="docutils literal notranslate"><span class="pre">retrieval</span></code> - which returns precision&#64;k of the documents retrieved for provided inputs.</p>
<p>The evaluation script expects paths to two files:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">evaluation_set</span></code> - a path to a file specifying the evaluation dataset, a single input per line.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gold_data_path</span></code> - a path to a file contaning ground truth answers for datapoints from the <code class="docutils literal notranslate"><span class="pre">evaluation_set</span></code>, a single output per line. Check below for expected formats of the gold data files.</p></li>
</ul>
<div class="section" id="retrieval-evaluation">
<h2>Retrieval evaluation<a class="headerlink" href="#retrieval-evaluation" title="Permalink to this headline">¶</a></h2>
<p>For <code class="docutils literal notranslate"><span class="pre">retrieval</span></code> evaluation, we expect a gold data file where each line will consist of a tab-separated list of document titles constituting positive contexts for respective datapoints from the <code class="docutils literal notranslate"><span class="pre">evaluation_set</span></code>. E.g. given a question <code class="docutils literal notranslate"><span class="pre">who</span> <span class="pre">sings</span> <span class="pre">does</span> <span class="pre">he</span> <span class="pre">love</span> <span class="pre">me</span> <span class="pre">with</span> <span class="pre">reba</span></code> in the <code class="docutils literal notranslate"><span class="pre">evaluation_set</span></code>, a respective ground truth line could look as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Does</span> <span class="n">He</span> <span class="n">Love</span> <span class="n">You</span>	<span class="n">Does</span> <span class="n">He</span> <span class="n">Love</span> <span class="n">You</span>	<span class="n">Red</span> <span class="n">Sandy</span> <span class="n">Spika</span> <span class="n">dress</span> <span class="n">of</span> <span class="n">Reba</span> <span class="n">McEntire</span>	<span class="n">Greatest</span> <span class="n">Hits</span> <span class="n">Volume</span> <span class="n">Two</span> <span class="p">(</span><span class="n">Reba</span> <span class="n">McEntire</span> <span class="n">album</span><span class="p">)</span>	<span class="n">Shoot</span> <span class="k">for</span> <span class="n">the</span> <span class="n">Moon</span> <span class="p">(</span><span class="n">album</span><span class="p">)</span>
</pre></div>
</div>
<p>We demonstrate how to evaluate retrieval against DPR evaluation data. You can download respective files from links listed <a class="reference external" href="https://github.com/facebookresearch/DPR/blob/master/data/download_data.py#L39-L45">here</a>.</p>
<ol>
<li><p>Download and unzip the gold data file. We use the <code class="docutils literal notranslate"><span class="pre">biencoder-nq-dev</span></code> from https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz <span class="o">&amp;&amp;</span> gzip -d biencoder-nq-dev.json.gz
</pre></div>
</div>
</li>
<li><p>Parse the unziped file using the <code class="docutils literal notranslate"><span class="pre">parse_dpr_relevance_data.py</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir output <span class="c1"># or wherever you want to save this</span>
python examples/research_projects/rag/parse_dpr_relevance_data.py <span class="se">\</span>
    --src_path biencoder-nq-dev.json <span class="se">\</span>
    --evaluation_set output/biencoder-nq-dev.questions <span class="se">\</span>
    --gold_data_path output/biencoder-nq-dev.pages
</pre></div>
</div>
</li>
<li><p>Run evaluation:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python examples/research_projects/rag/eval_rag.py <span class="se">\</span>
    --model_name_or_path facebook/rag-sequence-nq <span class="se">\</span>
    --model_type rag_sequence <span class="se">\</span>
    --evaluation_set output/biencoder-nq-dev.questions <span class="se">\</span>
    --gold_data_path output/biencoder-nq-dev.pages <span class="se">\</span>
    --predictions_path output/retrieval_preds.tsv  <span class="se">\</span>
    --eval_mode retrieval <span class="se">\</span>
    --k <span class="m">1</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># EXPLANATION</span>
 python examples/research_projects/rag/eval_rag.py <span class="se">\</span>
     --model_name_or_path facebook/rag-sequence-nq <span class="se">\ </span><span class="c1"># model name or path of the model we&#39;re evaluating</span>
     --model_type rag_sequence <span class="se">\ </span><span class="c1"># RAG model type (rag_token or rag_sequence)</span>
     --evaluation_set output/biencoder-nq-dev.questions <span class="se">\ </span><span class="c1"># an input dataset for evaluation</span>
     --gold_data_path poutput/biencoder-nq-dev.pages <span class="se">\ </span><span class="c1"># a dataset containing ground truth answers for samples from the evaluation_set</span>
     --predictions_path output/retrieval_preds.tsv  <span class="se">\ </span><span class="c1"># name of file where predictions will be stored</span>
     --eval_mode retrieval <span class="se">\ </span><span class="c1"># indicates whether we&#39;re performing retrieval evaluation or e2e evaluation</span>
     --k <span class="m">1</span> <span class="c1"># parameter k for the precision@k metric</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="end-to-end-evaluation">
<h2>End-to-end evaluation<a class="headerlink" href="#end-to-end-evaluation" title="Permalink to this headline">¶</a></h2>
<p>We support two formats of the gold data file (controlled by the <code class="docutils literal notranslate"><span class="pre">gold_data_mode</span></code> parameter):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">qa</span></code> - where a single line has the following format: <code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">[tab]</span> <span class="pre">output_list</span></code>, e.g.:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">who</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">owner</span> <span class="n">of</span> <span class="n">reading</span> <span class="n">football</span> <span class="n">club</span>	<span class="p">[</span><span class="s1">&#39;Xiu Li Dai&#39;</span><span class="p">,</span> <span class="s1">&#39;Dai Yongge&#39;</span><span class="p">,</span> <span class="s1">&#39;Dai Xiuli&#39;</span><span class="p">,</span> <span class="s1">&#39;Yongge Dai&#39;</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ans</span></code> - where a single line contains a single expected answer, e.g.:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Xiu</span> <span class="n">Li</span> <span class="n">Dai</span>
</pre></div>
</div>
<p>Predictions of the model for the samples from the <code class="docutils literal notranslate"><span class="pre">evaluation_set</span></code> will be saved under the path specified by the <code class="docutils literal notranslate"><span class="pre">predictions_path</span></code> parameter.
If this path already exists, the script will use saved predictions to calculate metrics.
Add <code class="docutils literal notranslate"><span class="pre">--recalculate</span></code> parameter to force the script to perform inference from scratch.</p>
<p>An example e2e evaluation run could look as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python examples/research_projects/rag/eval_rag.py <span class="se">\</span>
    --model_name_or_path facebook/rag-sequence-nq <span class="se">\</span>
    --model_type rag_sequence <span class="se">\</span>
    --evaluation_set path/to/test.source <span class="se">\</span>
    --gold_data_path path/to/gold_data <span class="se">\</span>
    --predictions_path path/to/e2e_preds.txt <span class="se">\</span>
    --eval_mode e2e <span class="se">\</span>
    --gold_data_mode qa <span class="se">\</span>
    --n_docs <span class="m">5</span> <span class="se">\ </span><span class="c1"># You can experiment with retrieving different number of documents at evaluation time</span>
    --print_predictions <span class="se">\</span>
    --recalculate <span class="se">\ </span><span class="c1"># adding this parameter will force recalculating predictions even if predictions_path already exists</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="use-your-own-knowledge-source">
<h1>Use your own knowledge source<a class="headerlink" href="#use-your-own-knowledge-source" title="Permalink to this headline">¶</a></h1>
<p>By default, RAG uses the English Wikipedia as a knowledge source, known as the ‘wiki_dpr’ dataset.
With <code class="docutils literal notranslate"><span class="pre">use_custom_knowledge_dataset.py</span></code> you can build your own knowledge source, <em>e.g.</em> for RAG.</p>
<p>For instance, if documents are serialized as tab-separated csv files with the columns “title” and “text”, one can use <code class="docutils literal notranslate"><span class="pre">use_own_knowledge_dataset.py</span></code> as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python examples/research_projects/rag/use_own_knowledge_dataset.py <span class="se">\</span>
    --csv_path path/to/my_csv <span class="se">\</span>
    --output_dir path/to/my_knowledge_dataset <span class="se">\</span>
</pre></div>
</div>
<p>The created outputs in <code class="docutils literal notranslate"><span class="pre">path/to/my_knowledge_dataset</span></code> can then be used to finetune RAG as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python examples/research_projects/rag/finetune_rag.py <span class="se">\</span>
    --data_dir <span class="nv">$DATA_DIR</span> <span class="se">\</span>
    --output_dir <span class="nv">$OUTPUT_DIR</span> <span class="se">\</span>
    --model_name_or_path <span class="nv">$MODEL_NAME_OR_PATH</span> <span class="se">\</span>
    --model_type rag_sequence <span class="se">\</span>
    --fp16 <span class="se">\</span>
    --gpus <span class="m">8</span>
    --index_name custom
    --passages_path path/to/data/my_knowledge_dataset
    --index_path path/to/my_knowledge_dataset_hnsw_index.faiss
</pre></div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Intel® LPOT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>