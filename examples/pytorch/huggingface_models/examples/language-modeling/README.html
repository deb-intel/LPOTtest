

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Language model training &mdash; IntelÂ® Low Precision Optimization Tool  documentation</title>
  

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../../index.html" class="icon icon-home"> IntelÂ® Low Precision Optimization Tool
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../welcome.html">Introduction to Intel LPOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docs/introduction.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docs/index.html">Developer Docs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../CONTRIBUTING.html">Contributing guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../legal_information.html">Legal Information {#legal_information}</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../legal_information.html#trademark-information">Trademark Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../SECURITY.html">Security Policy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">IntelÂ® Low Precision Optimization Tool</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Language model training</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../../_sources/examples/pytorch/huggingface_models/examples/language-modeling/README.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--><div class="section" id="language-model-training">
<h1>Language model training<a class="headerlink" href="#language-model-training" title="Permalink to this headline">Â¶</a></h1>
<p>Fine-tuning (or training from scratch) the library models for language modeling on a text dataset for GPT, GPT-2,
ALBERT, BERT, DistilBERT, RoBERTa, XLNetâ€¦ GPT and GPT-2 are trained or fine-tuned using a causal language modeling
(CLM) loss while ALBERT, BERT, DistilBERT and RoBERTa are trained or fine-tuned using a masked language modeling (MLM)
loss. XLNet uses permutation language modeling (PLM), you can find more information about the differences between those
objectives in our <a class="reference external" href="https://huggingface.co/transformers/model_summary.html">model summary</a>.</p>
<p>These scripts leverage the ðŸ¤— Datasets library and the Trainer API. You can easily customize them to your needs if you
need extra processing on your datasets.</p>
<p><strong>Note:</strong> The old script <code class="docutils literal notranslate"><span class="pre">run_language_modeling.py</span></code> is still available <a class="reference external" href="https://github.com/huggingface/transformers/blob/master/examples/legacy/run_language_modeling.py">here</a>.</p>
<p>The following examples, will run on a datasets hosted on our <a class="reference external" href="https://huggingface.co/datasets">hub</a> or with your own
text files for training and validation. We give examples of both below.</p>
<div class="section" id="gpt-2-gpt-and-causal-language-modeling">
<h2>GPT-2/GPT and causal language modeling<a class="headerlink" href="#gpt-2-gpt-and-causal-language-modeling" title="Permalink to this headline">Â¶</a></h2>
<p>The following example fine-tunes GPT-2 on WikiText-2. Weâ€™re using the raw WikiText-2 (no tokens were replaced before
the tokenization). The loss here is that of causal language modeling.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python run_clm.py <span class="se">\</span>
    --model_name_or_path gpt2 <span class="se">\</span>
    --dataset_name wikitext <span class="se">\</span>
    --dataset_config_name wikitext-2-raw-v1 <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --output_dir /tmp/test-clm
</pre></div>
</div>
<p>This takes about half an hour to train on a single K80 GPU and about one minute for the evaluation to run. It reaches
a score of ~20 perplexity once fine-tuned on the dataset.</p>
<p>To run on your own training and validation files, use the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python run_clm.py <span class="se">\</span>
    --model_name_or_path gpt2 <span class="se">\</span>
    --train_file path_to_train_file <span class="se">\</span>
    --validation_file path_to_validation_file <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --output_dir /tmp/test-clm
</pre></div>
</div>
</div>
<div class="section" id="roberta-bert-distilbert-and-masked-language-modeling">
<h2>RoBERTa/BERT/DistilBERT and masked language modeling<a class="headerlink" href="#roberta-bert-distilbert-and-masked-language-modeling" title="Permalink to this headline">Â¶</a></h2>
<p>The following example fine-tunes RoBERTa on WikiText-2. Here too, weâ€™re using the raw WikiText-2. The loss is different
as BERT/RoBERTa have a bidirectional mechanism; weâ€™re therefore using the same loss that was used during their
pre-training: masked language modeling.</p>
<p>In accordance to the RoBERTa paper, we use dynamic masking rather than static masking. The model may, therefore,
converge slightly slower (over-fitting takes more epochs).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python run_mlm.py <span class="se">\</span>
    --model_name_or_path roberta-base <span class="se">\</span>
    --dataset_name wikitext <span class="se">\</span>
    --dataset_config_name wikitext-2-raw-v1 <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --output_dir /tmp/test-mlm
</pre></div>
</div>
<p>To run on your own training and validation files, use the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python run_mlm.py <span class="se">\</span>
    --model_name_or_path roberta-base <span class="se">\</span>
    --train_file path_to_train_file <span class="se">\</span>
    --validation_file path_to_validation_file <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --output_dir /tmp/test-mlm
</pre></div>
</div>
<p>If your dataset is organized with one sample per line, you can use the <code class="docutils literal notranslate"><span class="pre">--line_by_line</span></code> flag (otherwise the script
concatenates all texts and then splits them in blocks of the same length).</p>
<p><strong>Note:</strong> On TPU, you should use the flag <code class="docutils literal notranslate"><span class="pre">--pad_to_max_length</span></code> in conjunction with the <code class="docutils literal notranslate"><span class="pre">--line_by_line</span></code> flag to make
sure all your batches have the same length.</p>
</div>
<div class="section" id="whole-word-masking">
<h2>Whole word masking<a class="headerlink" href="#whole-word-masking" title="Permalink to this headline">Â¶</a></h2>
<p>This part was moved to <code class="docutils literal notranslate"><span class="pre">examples/research_projects/mlm_wwm</span></code>.</p>
</div>
<div class="section" id="xlnet-and-permutation-language-modeling">
<h2>XLNet and permutation language modeling<a class="headerlink" href="#xlnet-and-permutation-language-modeling" title="Permalink to this headline">Â¶</a></h2>
<p>XLNet uses a different training objective, which is permutation language modeling. It is an autoregressive method
to learn bidirectional contexts by maximizing the expected likelihood over all permutations of the input
sequence factorization order.</p>
<p>We use the <code class="docutils literal notranslate"><span class="pre">--plm_probability</span></code> flag to define the ratio of length of a span of masked tokens to surrounding
context length for permutation language modeling.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">--max_span_length</span></code> flag may also be used to limit the length of a span of masked tokens used
for permutation language modeling.</p>
<p>Here is how to fine-tune XLNet on wikitext-2:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python run_plm.py <span class="se">\</span>
    --model_name_or_path<span class="o">=</span>xlnet-base-cased <span class="se">\</span>
    --dataset_name wikitext <span class="se">\</span>
    --dataset_config_name wikitext-2-raw-v1 <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --output_dir /tmp/test-plm
</pre></div>
</div>
<p>To fine-tune it on your own training and validation file, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python run_plm.py <span class="se">\</span>
    --model_name_or_path<span class="o">=</span>xlnet-base-cased <span class="se">\</span>
    --train_file path_to_train_file <span class="se">\</span>
    --validation_file path_to_validation_file <span class="se">\</span>
    --do_train <span class="se">\</span>
    --do_eval <span class="se">\</span>
    --output_dir /tmp/test-plm
</pre></div>
</div>
<p>If your dataset is organized with one sample per line, you can use the <code class="docutils literal notranslate"><span class="pre">--line_by_line</span></code> flag (otherwise the script
concatenates all texts and then splits them in blocks of the same length).</p>
<p><strong>Note:</strong> On TPU, you should use the flag <code class="docutils literal notranslate"><span class="pre">--pad_to_max_length</span></code> in conjunction with the <code class="docutils literal notranslate"><span class="pre">--line_by_line</span></code> flag to make
sure all your batches have the same length.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, IntelÂ® LPOT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>