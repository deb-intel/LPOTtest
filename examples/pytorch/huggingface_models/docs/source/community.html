

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Community &mdash; IntelÂ® Low Precision Optimization Tool  documentation</title>
  

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../../index.html" class="icon icon-home"> IntelÂ® Low Precision Optimization Tool
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../welcome.html">Introduction to Intel LPOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docs/introduction.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docs/index.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../CONTRIBUTING.html">Contributing Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../SECURITY.html">Security Policy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">IntelÂ® Low Precision Optimization Tool</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Community</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../../_sources/examples/pytorch/huggingface_models/docs/source/community.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="community">
<h1>Community<a class="headerlink" href="#community" title="Permalink to this headline">Â¶</a></h1>
<p>This page regroups resources around ðŸ¤— Transformers developed by the community.</p>
<div class="section" id="community-resources">
<h2>Community resources:<a class="headerlink" href="#community-resources" title="Permalink to this headline">Â¶</a></h2>
<table border="1" class="docutils">
<thead>
<tr>
<th align="left">Resource</th>
<th align="left">Description</th>
<th align="right">Author</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><a href="https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards">Hugging Face Transformers Glossary Flashcards</a></td>
<td align="left">A set of flashcards based on the <a href="https://huggingface.co/transformers/master/glossary.html">Transformers Docs Glossary</a> that has been put into a form which can be easily learnt/revised using <a href="https://apps.ankiweb.net/">Anki </a> an open source, cross platform app specifically designed for long term knowledge retention. See this <a href="https://www.youtube.com/watch?v=Dji_h7PILrw">Introductory video on how to use the flashcards</a>.</td>
<td align="right"><a href="https://www.darigovresearch.com/">Darigov Research</a></td>
</tr>
</tbody>
</table></div>
<div class="section" id="community-notebooks">
<h2>Community notebooks:<a class="headerlink" href="#community-notebooks" title="Permalink to this headline">Â¶</a></h2>
<table border="1" class="docutils">
<thead>
<tr>
<th align="left">Notebook</th>
<th align="left">Description</th>
<th align="left">Author</th>
<th align="right"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><a href="https://github.com/snapthat/TF-T5-text-to-text">Train T5 in Tensorflow 2 </a></td>
<td align="left">How to train T5 for any task using Tensorflow 2. This notebook demonstrates a Question &amp; Answer task implemented in Tensorflow 2 using SQUAD</td>
<td align="left"><a href="https://github.com/HarrisDePerceptron">Muhammad Harris</a></td>
<td align="right"><a href="https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb">Train T5 on TPU</a></td>
<td align="left">How to train T5 on SQUAD with Transformers and Nlp</td>
<td align="left"><a href="https://github.com/patil-suraj">Suraj Patil</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb">Fine-tune T5 for Classification and Multiple Choice</a></td>
<td align="left">How to fine-tune T5 for classification and multiple choice tasks using a text-to-text format with PyTorch Lightning</td>
<td align="left"><a href="https://github.com/patil-suraj">Suraj Patil</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb">Fine-tune DialoGPT on New Datasets and Languages</a></td>
<td align="left">How to fine-tune the DialoGPT model on a new dataset for open-dialog conversational chatbots</td>
<td align="left"><a href="https://github.com/ncoop57">Nathan Cooper</a></td>
<td align="right"><a href="https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb">Long Sequence Modeling with Reformer</a></td>
<td align="left">How to train on sequences as long as 500,000 tokens with Reformer</td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/ohmeow/ohmeow_website/blob/master/_notebooks/2020-05-23-text-generation-with-blurr.ipynb">Fine-tune BART for Summarization</a></td>
<td align="left">How to fine-tune BART for summarization with fastai using blurr</td>
<td align="left"><a href="https://ohmeow.com/">Wayde Gilliam</a></td>
<td align="right"><a href="https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/_notebooks/2020-05-23-text-generation-with-blurr.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb">Fine-tune a pre-trained Transformer on anyone's tweets</a></td>
<td align="left">How to generate tweets in the style of your favorite Twitter account by fine-tuning a GPT-2 model</td>
<td align="left"><a href="https://github.com/borisdayma">Boris Dayma</a></td>
<td align="right"><a href="https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb">Optimize ðŸ¤— Hugging Face models with Weights &amp; Biases</a></td>
<td align="left">A complete tutorial showcasing W&amp;B integration with Hugging Face</td>
<td align="left"><a href="https://github.com/borisdayma">Boris Dayma</a></td>
<td align="right"><a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb">Pretrain Longformer</a></td>
<td align="left">How to build a "long" version of existing pretrained models</td>
<td align="left"><a href="https://beltagy.net">Iz Beltagy</a></td>
<td align="right"><a href="https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb">Fine-tune Longformer for QA</a></td>
<td align="left">How to fine-tune longformer model for QA task</td>
<td align="left"><a href="https://github.com/patil-suraj">Suraj Patil</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb">Evaluate Model with ðŸ¤—nlp</a></td>
<td align="left">How to evaluate longformer on TriviaQA with <code>nlp</code></td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb">Fine-tune T5 for Sentiment Span Extraction</a></td>
<td align="left">How to fine-tune T5 for sentiment span extraction using a text-to-text format with PyTorch Lightning</td>
<td align="left"><a href="https://github.com/enzoampil">Lorenzo Ampil</a></td>
<td align="right"><a href="https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb">Fine-tune DistilBert for Multiclass Classification</a></td>
<td align="left">How to fine-tune DistilBert for multiclass classification with PyTorch</td>
<td align="left"><a href="https://github.com/abhimishra91">Abhishek Kumar Mishra</a></td>
<td align="right"><a href="https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb">Fine-tune BERT for Multi-label Classification</a></td>
<td align="left">How to fine-tune BERT for multi-label classification using PyTorch</td>
<td align="left"><a href="https://github.com/abhimishra91">Abhishek Kumar Mishra</a></td>
<td align="right"><a href="https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb">Fine-tune T5 for Summarization</a></td>
<td align="left">How to fine-tune T5 for summarization in PyTorch and track experiments with WandB</td>
<td align="left"><a href="https://github.com/abhimishra91">Abhishek Kumar Mishra</a></td>
<td align="right"><a href="https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb">Speed up Fine-Tuning in Transformers with Dynamic Padding / Bucketing</a></td>
<td align="left">How to speed up fine-tuning by a factor of 2 using dynamic padding / bucketing</td>
<td align="left"><a href="https://github.com/pommedeterresautee">Michael Benesty</a></td>
<td align="right"><a href="https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb">Pretrain Reformer for Masked Language Modeling</a></td>
<td align="left">How to train a Reformer model with bi-directional self-attention layers</td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb">Expand and Fine Tune Sci-BERT</a></td>
<td align="left">How to increase vocabulary of a pretrained SciBERT model from AllenAI on the CORD dataset and pipeline it.</td>
<td align="left"><a href="https://github.com/lordtt13">Tanmay Thakur</a></td>
<td align="right"><a href="https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb">Fine Tune BlenderBotSmall for Summarization using the Trainer API</a></td>
<td align="left">How to fine tune BlenderBotSmall for summarization on a custom dataset, using the Trainer API.</td>
<td align="left"><a href="https://github.com/lordtt13">Tanmay Thakur</a></td>
<td align="right"><a href="https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb">Fine-tune Electra and interpret with Integrated Gradients</a></td>
<td align="left">How to fine-tune Electra for sentiment analysis and interpret predictions with Captum Integrated Gradients</td>
<td align="left"><a href="https://elsanns.github.io">Eliza Szczechla</a></td>
<td align="right"><a href="https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb">fine-tune a non-English GPT-2 Model with Trainer class</a></td>
<td align="left">How to fine-tune a non-English GPT-2 Model with Trainer class</td>
<td align="left"><a href="https://www.philschmid.de">Philipp Schmid</a></td>
<td align="right"><a href="https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb">Fine-tune a DistilBERT Model for Multi Label Classification task</a></td>
<td align="left">How to fine-tune a DistilBERT Model for Multi Label Classification task</td>
<td align="left"><a href="https://github.com/DhavalTaunk08">Dhaval Taunk</a></td>
<td align="right"><a href="https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb">Fine-tune ALBERT for sentence-pair classification</a></td>
<td align="left">How to fine-tune an ALBERT model or another BERT-based model for the sentence-pair classification task</td>
<td align="left"><a href="https://github.com/NadirEM">Nadir El Manouzi</a></td>
<td align="right"><a href="https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb">Fine-tune Roberta for sentiment analysis</a></td>
<td align="left">How to fine-tune an Roberta model for sentiment analysis</td>
<td align="left"><a href="https://github.com/DhavalTaunk08">Dhaval Taunk</a></td>
<td align="right"><a href="https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/flexudy-pipe/qugeev">Evaluating Question Generation Models</a></td>
<td align="left">How accurate are the answers to questions generated by your seq2seq transformer model?</td>
<td align="left"><a href="https://github.com/zolekode">Pascal Zoleko</a></td>
<td align="right"><a href="https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb">Classify text with DistilBERT and Tensorflow</a></td>
<td align="left">How to fine-tune DistilBERT for text classification in TensorFlow</td>
<td align="left"><a href="https://github.com/peterbayerle">Peter Bayerle</a></td>
<td align="right"><a href="https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb">Leverage BERT for Encoder-Decoder Summarization on CNN/Dailymail</a></td>
<td align="left">How to warm-start a <em>EncoderDecoderModel</em> with a <em>bert-base-uncased</em> checkpoint for summarization on CNN/Dailymail</td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb">Leverage RoBERTa for Encoder-Decoder Summarization on BBC XSum</a></td>
<td align="left">How to warm-start a shared <em>EncoderDecoderModel</em> with a <em>roberta-base</em> checkpoint for summarization on BBC/XSum</td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb">Fine-tune TAPAS on Sequential Question Answering (SQA)</a></td>
<td align="left">How to fine-tune <em>TapasForQuestionAnswering</em> with a <em>tapas-base</em> checkpoint on the Sequential Question Answering (SQA) dataset</td>
<td align="left"><a href="https://github.com/nielsrogge">Niels Rogge</a></td>
<td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb">Evaluate TAPAS on Table Fact Checking (TabFact)</a></td>
<td align="left">How to evaluate a fine-tuned <em>TapasForSequenceClassification</em> with a <em>tapas-base-finetuned-tabfact</em> checkpoint using a combination of the ðŸ¤— datasets and ðŸ¤— transformers libraries</td>
<td align="left"><a href="https://github.com/nielsrogge">Niels Rogge</a></td>
<td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb">Fine-tuning mBART for translation</a></td>
<td align="left">How to fine-tune mBART using Seq2SeqTrainer for Hindi to English translation</td>
<td align="left"><a href="https://github.com/vasudevgupta7">Vasudev Gupta</a></td>
<td align="right"><a href="https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb">Fine-tune LayoutLM on FUNSD (a form understanding dataset)</a></td>
<td align="left">How to fine-tune <em>LayoutLMForTokenClassification</em> on the FUNSD dataset for information extraction from scanned documents</td>
<td align="left"><a href="https://github.com/nielsrogge">Niels Rogge</a></td>
<td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb">Fine-Tune DistilGPT2 and Generate Text</a></td>
<td align="left">How to fine-tune DistilGPT2 and generate text</td>
<td align="left"><a href="https://github.com/tripathiaakash">Aakash Tripathi</a></td>
<td align="right"><a href="https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb">Fine-Tune LED on up to 8K tokens</a></td>
<td align="left">How to fine-tune LED on pubmed for long-range summarization</td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb">Evaluate LED on Arxiv</a></td>
<td align="left">How to effectively evaluate LED on long-range summarization</td>
<td align="left"><a href="https://github.com/patrickvonplaten">Patrick von Platen</a></td>
<td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
<tr>
<td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb">Fine-tune LayoutLM on RVL-CDIP (a document image classification dataset)</a></td>
<td align="left">How to fine-tune <em>LayoutLMForSequenceClassification</em> on the RVL-CDIP dataset for scanned document classification</td>
<td align="left"><a href="https://github.com/nielsrogge">Niels Rogge</a></td>
<td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></td>
</tr>
</tbody>
</table></div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, IntelÂ® LPOT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>