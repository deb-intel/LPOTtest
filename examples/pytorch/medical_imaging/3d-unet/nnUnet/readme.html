

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>nnU-Net &mdash; Intel® Low Precision Optimization Tool  documentation</title>
  

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../../index.html" class="icon icon-home"> Intel® Low Precision Optimization Tool
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../welcome.html">Introduction to Intel LPOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docs/introduction.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docs/doclist.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../releases-info.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../CONTRIBUTING.html">Contributing Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../SECURITY.html">Security Policy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">Intel® Low Precision Optimization Tool</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>nnU-Net</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../../_sources/examples/pytorch/medical_imaging/3d-unet/nnUnet/readme.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="nnu-net">
<h1>nnU-Net<a class="headerlink" href="#nnu-net" title="Permalink to this headline">¶</a></h1>
<p>In 3D biomedical image segmentation, dataset properties like imaging modality, image sizes, voxel spacings, class
ratios etc vary drastically.
For example, images in the <a class="reference external" href="https://competitions.codalab.org/competitions/17094">Liver and Liver Tumor Segmentation Challenge dataset</a>
are computed tomography (CT) scans, about 512x512x512 voxels large, have isotropic voxel spacings and their
intensity values are quantitative (Hounsfield Units).
The <a class="reference external" href="https://acdc.creatis.insa-lyon.fr/">Automated Cardiac Diagnosis Challenge dataset</a> on the other hand shows cardiac
structures in cine MRI with a typical image shape of 10x320x320 voxels, highly anisotropic voxel spacings and
qualitative intensity values. In addition, the ACDC dataset suffers from slice misalignments and a heterogeneity of
out-of-plane spacings which can cause severe interpolation artifacts if not handled properly.</p>
<p>In current research practice, segmentation pipelines are designed manually and with one specific dataset in mind.
Hereby, many pipeline settings depend directly or indirectly on the properties of the dataset
and display a complex co-dependence: image size, for example, affects the patch size, which in
turn affects the required receptive field of the network, a factor that itself influences several other
hyperparameters in the pipeline. As a result, pipelines that were developed on one (type of) dataset are inherently
incomaptible with other datasets in the domain.</p>
<p><strong>nnU-Net is the first segmentation method that is designed to deal with the dataset diversity found in the somain. It
condenses and automates the keys decisions for designing a successful segmentation pipeline for any given dataset.</strong></p>
<p>nnU-Net makes the following contributions to the field:</p>
<ol class="simple">
<li><p><strong>Standardized baseline:</strong> nnU-Net is the first standardized deep learning benchmark in biomedical segmentation.
Without manual effort, researchers can compare their algorithms against nnU-Net on an arbitrary number of datasets
to provide meaningful evidence for proposed improvements.</p></li>
<li><p><strong>Out-of-the-box segmentation method:</strong> nnU-Net is the first plug-and-play tool for state-of-the-art biomedical
segmentation. Inexperienced users can use nnU-Net out of the box for their custom 3D segmentation problem without
need for manual intervention.</p></li>
<li><p><strong>Framework:</strong> nnU-Net is a framework for fast and effective development of segmentation methods. Due to its modular
structure, new architectures and methods can easily be integrated into nnU-Net. Researchers can then benefit from its
generic nature to roll out and evaluate their modifications on an arbitrary number of datasets in a
standardized environment.</p></li>
</ol>
<p>For more information about nnU-Net, please read the following paper:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Fabian</span> <span class="n">Isensee</span><span class="p">,</span> <span class="n">Paul</span> <span class="n">F</span><span class="o">.</span> <span class="n">Jäger</span><span class="p">,</span> <span class="n">Simon</span> <span class="n">A</span><span class="o">.</span> <span class="n">A</span><span class="o">.</span> <span class="n">Kohl</span><span class="p">,</span> <span class="n">Jens</span> <span class="n">Petersen</span><span class="p">,</span> <span class="n">Klaus</span> <span class="n">H</span><span class="o">.</span> <span class="n">Maier</span><span class="o">-</span><span class="n">Hein</span> <span class="s2">&quot;Automated Design of Deep Learning </span>
<span class="n">Methods</span> <span class="k">for</span> <span class="n">Biomedical</span> <span class="n">Image</span> <span class="n">Segmentation</span><span class="s2">&quot; arXiv preprint arXiv:1904.08128 (2020).</span>
</pre></div>
</div>
<p>Please also cite this paper if you are using nnU-Net for your research!</p>
</div>
<div class="section" id="table-of-contents">
<h1>Table of Contents<a class="headerlink" href="#table-of-contents" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference external" href="#installation">Installation</a></p></li>
<li><p><a class="reference external" href="#usage">Usage</a></p>
<ul>
<li><p><a class="reference external" href="#how-to-run-nnu-net-on-a-new-datasets">How to run nnU-Net on a new datasets</a></p>
<ul>
<li><p><a class="reference external" href="#dataset-conversion">Dataset conversion</a></p></li>
<li><p><a class="reference external" href="#experiment-planning-and-preprocessing">Experiment planning and preprocessing</a></p></li>
<li><p><a class="reference external" href="#model-training">Model training</a></p>
<ul>
<li><p><a class="reference external" href="#2d-u-net">2D U-Net</a></p></li>
<li><p><a class="reference external" href="#3d-full-resolution-u-net">3D full resolution U-Net</a></p></li>
<li><p><a class="reference external" href="#3d-u-net-cascade">3D U-Net cascade</a></p>
<ul>
<li><p><a class="reference external" href="#3d-low-resolution-u-net">3D low resolution U-Net</a></p></li>
<li><p><a class="reference external" href="#3d-full-resolution-u-net-1">3D full resolution U-Net</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#multi-gpu-training">Multi GPU training</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#identifying-the-best-u-net-configuration">Identifying the best U-Net configuration(s)</a></p></li>
<li><p><a class="reference external" href="#run-inference">Run inference</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#how-to-run-inference-with-pretrained-models">How to run inference with pretrained models</a></p></li>
<li><p><a class="reference external" href="#Examples">Examples</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#extending-changing-nnu-net">Extending/Changing nnU-Net</a></p></li>
<li><p><a class="reference external" href="#faq">FAQ</a></p></li>
</ul>
<p>ecotrust-canada.github.io/markdown-toc/</p>
</div>
<div class="section" id="installation">
<h1>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h1>
<p>nnU-Net is only tested on Linux (Ubuntu). It may work on other operating systems as well but we do not guarantee that it will.</p>
<p>nnU-Net requires a GPU! For inference, the GPU should have 4 GB of VRAM. For training nnU-Net models the GPU should have at
least 11 GB (such as the RTX 2080ti). Due to the use of mixed precision, fastest training times are achieved with the
Volta architecture (Titan V, V100 GPUs) (tensorcore acceleration for 3D convolutions does not yet work on Turing-based GPUs).</p>
<p>We very strongly recommend you install nnU-Net in a virtual environment.
<a class="reference external" href="https://linoxide.com/linux-how-to/setup-python-virtual-environment-ubuntu/">Here is a quick how-to for Ubuntu.</a>.
Please do not use conda environments. This has caused multiple issues in the past.</p>
<p>Python 2 is deprecated and not supported. Please make sure you are using Python 3 :-)</p>
<ol>
<li><p>Install <a class="reference external" href="https://pytorch.org/get-started/locally/">PyTorch</a></p></li>
<li><p>Install <a class="reference external" href="https://github.com/NVIDIA/apex">Nvidia Apex</a>. Follow the instructions <a class="reference external" href="https://github.com/NVIDIA/apex#quick-start">here</a>.
You can skip this step if all you want to do is run inference with our pretrained models. Apex is required for
mixed precision training. (Please <strong>do not use</strong> <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">apex</span></code> - this will not install the correct package).
When installing apex, you have two choices (both are described on the apex website linked above!):</p>
<ol class="simple">
<li><p>Python-only installation:
This will not compile custom kernels and is a little bit slower than the other option (&lt;10%). But it is much easier to do,
which is why we recommend this option for less experienced users</p></li>
<li><p>Regular installation:
This gives more performance, but requires a CUDA toolkit installation. When installing pytorch, you must make sure to
select the Cuda version that matches the toolkit version you have installed. You can check which version you have by
running <code class="docutils literal notranslate"><span class="pre">nvcc</span> <span class="pre">--version</span></code>. You furthermore need to have the python3 dev libraries installed on your system. Follow the
instructions <a class="reference external" href="https://stackoverflow.com/questions/21530577/fatal-error-python-h-no-such-file-or-directory">here</a> for how to do this.
Only after these prerequisites are done you can install apex.
Note that pytorch will compile the kernels only for the type of GPU that is in your system. If you intend to swap
out your GPU (or are installing this in a cluster environment), run <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">TORCH_CUDA_ARCH_LIST=&quot;6.1;7.0;7.5&quot;</span></code>
prior to installing apex. This will tell pytorch to compile for all currently available GPU types.</p></li>
</ol>
</li>
<li><p>Install nnU-Net depending on your use case:</p>
<ol>
<li><p>For use as <strong>standardized baseline</strong>, <strong>out-of-the-box segmentation algorithm</strong> or for running <strong>inference with pretrained models</strong>:</p>
<p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">nnunet</span></code></p>
</li>
<li><p>For use as integrative <strong>framework</strong> (this will create a copy of the nnU-Net code on your computer so that you can modify it as needed):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/MIC-DKFZ/nnUNet.git
<span class="nb">cd</span> nnUNet
pip install -e .
</pre></div>
</div>
</li>
</ol>
</li>
<li><p>nnU-Net needs to know where you intend to save raw data, preprocessed data and trained models. For this you need to
set a few of environment variables. Please follow the instructions <a class="reference internal" href="documentation/setting_up_paths.html"><span class="doc">here</span></a>.</p></li>
<li><p>(OPTIONAL) Install <a class="reference external" href="https://github.com/waleedka/hiddenlayer">hiddenlayer</a>. hiddenlayer enables nnU-net to generate
plots of the network topologies it generates (see <a class="reference external" href="#model-training">Model training</a>). To install hiddenlayer,
run the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install --upgrade git+https://github.com/nanohanno/hiddenlayer.git@bugfix/get_trace_graph#egg<span class="o">=</span>hiddenlayer
</pre></div>
</div>
</li>
</ol>
<p>Installing nnU-Net will add several new commands to your terminal. These commands are used to run the entire nnU-Net
pipeline. You can execute them from any location on your system. All nnU-Net commands have the prefix <code class="docutils literal notranslate"><span class="pre">nnUNet_</span></code> for
easy identification.</p>
<p>Note that these commands simply execute python scripts. If you installed nnU-Net in a virtual environment, this
environment must be activated when executing the commands.</p>
<p>All nnU-Net commands have a <code class="docutils literal notranslate"><span class="pre">-h</span></code> option which gives information on how to use them.</p>
</div>
<div class="section" id="usage">
<h1>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h1>
<div class="section" id="how-to-run-nnu-net-on-a-new-dataset">
<h2>How to run nnU-Net on a new dataset<a class="headerlink" href="#how-to-run-nnu-net-on-a-new-dataset" title="Permalink to this headline">¶</a></h2>
<p>Given some dataset, nnU-Net fully automatically configures an entire segmentation pipeline that matches its properties.
nnU-Net covers the entire pipeline, from preprocessing to model configuration, model training, postprocessing
all the way to ensembling. After running nnU-Net, the trained model(s) can be applied to the test cases for inference.</p>
<div class="section" id="dataset-conversion">
<h3>Dataset conversion<a class="headerlink" href="#dataset-conversion" title="Permalink to this headline">¶</a></h3>
<p>nnU-Net expects datasets in a structured format. This format closely (but not entirely) follows the data structure of
the <a class="reference external" href="http://medicaldecathlon.com/">Medical Segmentation Decthlon</a>. Please read
<a class="reference internal" href="documentation/dataset_conversion.html"><span class="doc">this</span></a> for information on how to convert datasets to be compatible with nnU-Net.</p>
</div>
<div class="section" id="experiment-planning-and-preprocessing">
<h3>Experiment planning and preprocessing<a class="headerlink" href="#experiment-planning-and-preprocessing" title="Permalink to this headline">¶</a></h3>
<p>As a first step, nnU-Net extracts a dataset fingerprint (a set of dataset-specific properties such as
image sizes, voxel spacings, intensity information etc). This information is used to create three U-Net configurations:
a 2D U-Net, a 3D U-Net that operated on full resolution images as well as a 3D U-Net cascade where the first U-Net
creates a coarse segmentation map in downsampled images which is then refined by the second U-Net.</p>
<p>Provided that the requested raw dataset is located in the correct folder (<code class="docutils literal notranslate"><span class="pre">nnUNet_raw_data_base/nnUNet_raw_data/TaskXXX_MYTASK</span></code>,
also see <a class="reference internal" href="documentation/dataset_conversion.html"><span class="doc">here</span></a>), you can run this step with the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nnUNet_plan_and_preprocess -t XXX --verify_dataset_integrity
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">XXX</span></code> is the integer identifier associated with your Task name <code class="docutils literal notranslate"><span class="pre">TaskXXX_MYTASK</span></code>. You can pass several task IDs at once.</p>
<p>Running <code class="docutils literal notranslate"><span class="pre">nnUNet_plan_and_preprocess</span></code> will populate your folder with preprocessed data. You will find the output in
nnUNet_preprocessed/TaskXXX_MYTASK. <code class="docutils literal notranslate"><span class="pre">nnUNet_plan_and_preprocess</span></code> creates subfolders with preprocessed data for the 2D
U-Net as well as all applicable 3D U-Nets. It will also create ‘plans’ files (with the ending.pkl) for the 2D and
3D configurations. These files contain the generated segmentation pipeline configuration and will be read by the
nnUNetTrainer (see below). Note that the preprocessed data folder only contains the training cases.
The test images are not preprocessed (they are not looked at at all!). Their preprocessing happens on the fly during
inference.</p>
<p><code class="docutils literal notranslate"><span class="pre">--verify_dataset_integrity</span></code> should be run at least for the first time the command is run on a given dataset. This will execute some
checks on the dataset to ensure that it is compatible with nnU-Net. If this check has passed once, it can be
omitted in future runs. If you adhere to the dataset conversion guide (see above) then this should pass without issues :-)</p>
<p>Note that <code class="docutils literal notranslate"><span class="pre">nnUNet_plan_and_preprocess</span></code> accepts several additional input arguments. Running <code class="docutils literal notranslate"><span class="pre">-h</span></code> will list all of them
along with a description. If you run out of RAM during preprocessing, you may want to adapt the number of processes
used with the <code class="docutils literal notranslate"><span class="pre">-tl</span></code> and <code class="docutils literal notranslate"><span class="pre">-tf</span></code> options.</p>
<p>After <code class="docutils literal notranslate"><span class="pre">nnUNet_plan_and_preprocess</span></code> is completed, the U-Net configurations have been created and a preprocessed copy
of the data will be located at nnUNet_preprocessed/TaskXXX_MYTASK.</p>
</div>
<div class="section" id="model-training">
<h3>Model training<a class="headerlink" href="#model-training" title="Permalink to this headline">¶</a></h3>
<p>nnU-Net trains all U-Net configurations in a 5-fold cross-validation. This enables nnU-Net to determine the
postprocessing and ensembling (see next step) on the training dataset. Per default, all U-Net configurations need to
be run on a given dataset. There are, however situations in which only some configurations (and maybe even without
running the cross-validation) are desired. See <a class="reference external" href="#faq">FAQ</a> for more information.</p>
<p>Note that not all U-Net configurations are created for all datasets. In datasets with small image sizes, the U-Net
cascade is omitted because the patch size of the full resolution U-Net already covers a large part of the input images.</p>
<p>Training models is done with the <code class="docutils literal notranslate"><span class="pre">nnUNet_train</span></code> command. The general structure of the command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nnUNet_train CONFIGURATION TRAINER_CLASS_NAME TASK_NAME_OR_ID FOLD <span class="o">(</span>additional options<span class="o">)</span>
</pre></div>
</div>
<p>CONFIGURATION is a string that identifies the requested U-Net configuration. TRAINER_CLASS_NAME is the name of the
model trainer. If you implement custom trainers (nnU-Net as a framework) you can specify your custom trainer here.
TASK_NAME_OR_ID specifies what dataset should be trained on and FOLD specifies which fold of the 5-fold-crossvalidaton is trained.</p>
<p>nnU-Net stores a checkpoint every 50 epochs. If you need to continue a previous training, just add a <code class="docutils literal notranslate"><span class="pre">-c</span></code> to the
training command.</p>
<div class="section" id="d-u-net">
<h4>2D U-Net<a class="headerlink" href="#d-u-net" title="Permalink to this headline">¶</a></h4>
<p>For FOLD in [0, 1, 2, 3, 4], run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nnUNet_train 2d nnUNetTrainerV2 TaskXXX_MYTASK FOLD
</pre></div>
</div>
</div>
<div class="section" id="d-full-resolution-u-net">
<h4>3D full resolution U-Net<a class="headerlink" href="#d-full-resolution-u-net" title="Permalink to this headline">¶</a></h4>
<p>For FOLD in [0, 1, 2, 3, 4], run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nnUNet_train 3d_fullres nnUNetTrainerV2 TaskXXX_MYTASK FOLD
</pre></div>
</div>
</div>
<div class="section" id="d-u-net-cascade">
<h4>3D U-Net cascade<a class="headerlink" href="#d-u-net-cascade" title="Permalink to this headline">¶</a></h4>
<div class="section" id="d-low-resolution-u-net">
<h5>3D low resolution U-Net<a class="headerlink" href="#d-low-resolution-u-net" title="Permalink to this headline">¶</a></h5>
<p>For FOLD in [0, 1, 2, 3, 4], run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nnUNet_train 3d_lowres nnUNetTrainerV2 TaskXXX_MYTASK FOLD
</pre></div>
</div>
</div>
<div class="section" id="id1">
<h5>3D full resolution U-Net<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>For FOLD in [0, 1, 2, 3, 4], run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nnUNet_train 3d_cascade_fullres nnUNetTrainerV2CascadeFullRes TaskXXX_MYTASK FOLD
</pre></div>
</div>
<p>Note that the 3D full resolution U-Net of the cascade requires the five folds of the low resolution U-Net to be
completed beforehand!</p>
<p>The trained models will we written to the RESULTS_FOLDER/nnUNet folder. Each training obtains an automatically generated
output folder name:</p>
<p>nnUNet_preprocessed/CONFIGURATION/TaskXXX_MYTASKNAME/TRAINER_CLASS_NAME__PLANS_FILE_NAME/FOLD</p>
<p>For Task002_Heart (from the MSD), for example, this looks like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>RESULTS_FOLDER/nnUNet/
├── 2d
│   └── Task02_Heart
│       └── nnUNetTrainerV2__nnUNetPlansv2.1
│           ├── fold_0
│           ├── fold_1
│           ├── fold_2
│           ├── fold_3
│           └── fold_4
├── 3d_cascade_fullres
├── 3d_fullres
│   └── Task02_Heart
│       └── nnUNetTrainerV2__nnUNetPlansv2.1
│           ├── fold_0
│           │   ├── debug.json
│           │   ├── model_best.model
│           │   ├── model_best.model.pkl
│           │   ├── model_final_checkpoint.model
│           │   ├── model_final_checkpoint.model.pkl
│           │   ├── network_architecture.pdf
│           │   ├── progress.png
│           │   └── validation_raw
│           │       ├── la_007.nii.gz
│           │       ├── la_007.pkl
│           │       ├── la_016.nii.gz
│           │       ├── la_016.pkl
│           │       ├── la_021.nii.gz
│           │       ├── la_021.pkl
│           │       ├── la_024.nii.gz
│           │       ├── la_024.pkl
│           │       ├── summary.json
│           │       └── validation_args.json
│           ├── fold_1
│           ├── fold_2
│           ├── fold_3
│           └── fold_4
└── 3d_lowres
</pre></div>
</div>
<p>Note that 3d_lowres and 3d_cascade_fullres are not populated because this dataset did not trigger the cascade. In each
model training output folder (each of the fold_x folder, 10 in total here), the following files will be created (only
shown for one folder above for brevity):</p>
<ul class="simple">
<li><p>debug.json: Contains a summary of blueprint and inferred parameters used for training this model. Not easy to read,
but very useful for debugging ;-)</p></li>
<li><p>model_best.model / model_best.model.pkl: checkpoint files of the best model identified during training. Not used right now.</p></li>
<li><p>model_final_checkpoint.model / model_final_checkpoint.model.pkl: checkpoint files of the final model (after training
has ended). This is what is used for both validation and inference.</p></li>
<li><p>network_architecture.pdf (only if hiddenlayer is installed!): a pdf document with a figure of the network architecture in it.</p></li>
<li><p>progress.png: A plot of the training (blue) and validation (red) loss during training. Also shows an approximation of
the evlauation metric (green). This approximation is the average Dice score of the foreground classes. It should,
however, only to be taken with a grain of salt because it is computed on randomly drawn patches from the validation
data at the end of each epoch, and the aggregation of TP, FP and FN for the Dice computation treats the patches as if
they all originate from the same volume (‘global Dice’; we do not compute a Dice for each validation case and then
average over all cases but pretend that there is only one validation case from which we sample patches). The reason for
this is that the ‘global Dice’ is easy to compute during training and is still quite useful to evaluate whether a model
is training at all or not. A proper validation is run at the end of the training.</p></li>
<li><p>validation_raw: in this folder are the predicted validation cases after the training has finished. The summary.json
contains the validation metrics (a mean over all cases is provided at the end of the file).</p></li>
</ul>
<p>During training it is often useful to watch the progress. We therefore recommend that you have a look at the generated
progress.png when running the first training. It will be updated after each epoch.</p>
</div>
</div>
<div class="section" id="multi-gpu-training">
<h4>Multi GPU training<a class="headerlink" href="#multi-gpu-training" title="Permalink to this headline">¶</a></h4>
<p><strong>Multi GPU training is experimental and NOT RECOMMENDED!</strong></p>
<p>nnU-Net supports two different multi-GPU implementation: DataParallel (DP) and Distributed Data Parallel (DDP)
(but currently only on one host!). DDP is faster than DP and should be preferred if possible. However, if you did not
install nnunet as a framework (meaning you used the <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">nnunet</span></code> variant), DDP is not available. It requires a
different way of calling the correct python script (see below) which we cannot support from our terminal commands.</p>
<p>Distributed training currently only works for the basic trainers (2D, 3D full resolution and 3D low resolution) and not
for the second, high resolution U-Net of the cascade. The reason for this is that distributed training requires some
changes to the network and loss function, requiring a new nnUNet trainer class. This is, as of now, simply not
implemented for the cascade, but may be added in the future.</p>
<p>To run distributed training (DP), use the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2... nnUNet_train_DP CONFIGURATION nnUNetTrainerV2_DP TASK_NAME_OR_ID FOLD -gpus GPUS --dbs
</pre></div>
</div>
<p>Note that nnUNetTrainerV2 was replaced with nnUNetTrainerV2_DP. Just like before, CONFIGURATION can be 2d, 3d_lowres or
3d_fullres. TASK_NAME_OR_ID refers to the task you would like to train and FOLD is the fold of the cross-validation.
GPUS (integer value) specifies the number of GPUs you wish to train on. To specify which GPUs you want to use, please make use of the
CUDA_VISIBLE_DEVICES envorinment variable to specify the GPU ids (specify as many as you configure with -gpus GPUS).
–dbs, if set, will distribute the batch size across GPUs. So if nnUNet configures a batch size of 2 and you run on 2 GPUs
, each GPU will run with a batch size of 1. If you omit –dbs, each GPU will run with the full batch size (2 for each GPU
in this example for a total of batch size 4).</p>
<p>To run the DDP training you must have nnU-Net installed as a framework. Your current working directory must be the
nnunet folder (the one that has the dataset_conversion, evaluation, experiment_planning, … subfolders!). You can then run
the DDP training with the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2... python -m torch.distributed.launch --master_port<span class="o">=</span>XXXX --nproc_per_node<span class="o">=</span>Y run/run_training_DDP.py CONFIGURATION nnUNetTrainerV2_DDP TASK_NAME_OR_ID FOLD --dbs
</pre></div>
</div>
<p>XXXX must be an open port for process-process communication (something like 4321 will do on most systems). Y is the
number of GPUs you wish to use. Remember that we do not (yet) support distributed training across compute nodes. This
all happens on the same system. Again, you can use CUDA_VISIBLE_DEVICES=0,1,2 to control what GPUs are used.
If you run more than one DDP training on the same system (say you have 4 GPUs and you run two training with 2 GPUs each)
you need to specify a different –master_port for each training!</p>
<p><em>IMPORTANT!</em>
Multi-GPU training results in models that cannot be used for inference easily (as said above, all of this is experimental ;-) ).
After finishing the training of all folds, run <code class="docutils literal notranslate"><span class="pre">nnUNet_change_trainer_class</span></code> on the folder where the trained model is
(see <code class="docutils literal notranslate"><span class="pre">nnUNet_change_trainer_class</span> <span class="pre">-h</span></code> for instructions). After that you can run inference.</p>
</div>
</div>
<div class="section" id="identifying-the-best-u-net-configuration">
<h3>Identifying the best U-Net configuration<a class="headerlink" href="#identifying-the-best-u-net-configuration" title="Permalink to this headline">¶</a></h3>
<p>Once all models are trained, use the following
command to automatically determine what U-Net configuration(s) to use for test set prediction:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nnUNet_find_best_configuration -m 2d 3d_fullres 3d_lowres 3d_cascade_fullres -t XXX --strict
</pre></div>
</div>
<p>(all 5 folds need to be completed for all specified configurations!)</p>
<p>On datasets for which the cascade was not configured, use <code class="docutils literal notranslate"><span class="pre">-m</span> <span class="pre">2d</span> <span class="pre">3d_fullres</span></code> instead. If you wish to only explore some
subset of the configurations, you can specify that with the <code class="docutils literal notranslate"><span class="pre">-m</span></code> command. We recommend setting the
<code class="docutils literal notranslate"><span class="pre">--strict</span></code> (crash if one of the requested configurations is
missing) flag. Additional options are available (use <code class="docutils literal notranslate"><span class="pre">-h</span></code> for help).</p>
</div>
<div class="section" id="run-inference">
<h3>Run inference<a class="headerlink" href="#run-inference" title="Permalink to this headline">¶</a></h3>
<p>Remember that the data located in the input folder must adhere to the format specified
<a class="reference internal" href="documentation/data_format_inference.html"><span class="doc">here</span></a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">nnUNet_find_best_configuration</span></code> will print a string to the terminal with the inference commands you need to use.
The easiest way to run inference is to simply use these commands.</p>
<p>If you wish to manually specify the configuration(s) used for inference, use the following commands:</p>
<p>For each of the desired configurations, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nnUNet_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -t TASK_NAME_OR_ID -m CONFIGURATION --save_npz
</pre></div>
</div>
<p>Only specify <code class="docutils literal notranslate"><span class="pre">--save_npz</span></code> if you intend to use ensembling. <code class="docutils literal notranslate"><span class="pre">--save_npz</span></code> will make the command save the softmax
probabilities alongside of the predicted segmentation masks requiring a lot of disk space.</p>
<p>Please select a separate <code class="docutils literal notranslate"><span class="pre">OUTPUT_FOLDER</span></code> for each configuration!</p>
<p>If you wish to run ensembling, you can ensemble the predictions from several configurations with the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nnUNet_ensemble -f FOLDER1 FOLDER2 ... -o OUTPUT_FOLDER -pp POSTPROCESSING_FILE
</pre></div>
</div>
<p>You can specify an arbitrary number of folders, but remember that each folder needs to contain npz files that were
generated by <code class="docutils literal notranslate"><span class="pre">nnUNet_predict</span></code>. For ensembling you can also specify a file that tells the command how to postprocess.
These files are created when running <code class="docutils literal notranslate"><span class="pre">nnUNet_find_best_configuration</span></code> and are located in the respective trained model
directory (RESULTS_FOLDER/nnUNet/CONFIGURATION/TaskXXX_MYTASK/TRAINER_CLASS_NAME__PLANS_FILE_IDENTIFIER/postprocessing.json or
RESULTS_FOLDER/nnUNet/ensembles/TaskXXX_MYTASK/ensemble_X__Y__Z–X__Y__Z/postprocessing.json). You can also choose to
not provide a file (simply omit -pp) and nnU-Net will not run postprocessing.</p>
<p>Note that per default, inference will be done with all available folds. We very strongly recommend you use all 5 folds.
Thus, all 5 folds must have been trained prior to running inference. The list of available folds nnU-Net found will be
printed at the start of the inference.</p>
</div>
</div>
<div class="section" id="how-to-run-inference-with-pretrained-models">
<h2>How to run inference with pretrained models<a class="headerlink" href="#how-to-run-inference-with-pretrained-models" title="Permalink to this headline">¶</a></h2>
<p>(work in progress! Model weights have not been uploaded yet!)</p>
<p>Trained models for all challenges we participated in are publicly available. They can be downloaded and installed
directly with nnU-Net. Note that downloading a pretrained model will overwrite other models that were trained with
exactly the same configuration (2d, 3d_fullres, …), trainer (nnUNetTrainerV2) and plans.</p>
<p>To obtain a list of available models, as well as a short description, run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nnUNet_print_available_pretrained_models
</pre></div>
</div>
<p>You can then download models by specifying their task name. For the Liver and Liver Tumor Segmentation Challenge,
for example, this would be:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nnUNet_download_pretrained_model Task029_LiTS
</pre></div>
</div>
<p>After downloading is complete, you can use this model to run <a class="reference external" href="#run-inference">inference</a>. Keep in mind that each of
these models has specific data requirements (Task029_LiTS runs on abdominal CT scans, others require several image
modalities as input in a specific order).</p>
<p>When using the pretrained models you must adhere to the license of the dataset they are trained on! If you run
<code class="docutils literal notranslate"><span class="pre">nnUNet_download_pretrained_model</span></code> you will find a link where you can find the license for each dataset.</p>
</div>
<div class="section" id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<p>To get you started we compiled two simple to follow examples:</p>
<ul class="simple">
<li><p>run a training with the 3d full resolution U-Net on the Hippocampus dataset. See <a class="reference internal" href="documentation/training_example_Hippocampus.html"><span class="doc">here</span></a>.</p></li>
<li><p>run inference with nnU-Net’s pretrained models on the Prostate dataset. See <a class="reference internal" href="documentation/inference_example_Prostate.html"><span class="doc">here</span></a>.</p></li>
</ul>
<p>Usability not good enough? Let us know!</p>
</div>
</div>
<div class="section" id="extending-changing-nnu-net">
<h1>Extending/Changing nnU-Net<a class="headerlink" href="#extending-changing-nnu-net" title="Permalink to this headline">¶</a></h1>
<p>Please refer to <a class="reference internal" href="documentation/extending_nnunet.html"><span class="doc">this</span></a> guide.</p>
</div>
<div class="section" id="faq">
<h1>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline">¶</a></h1>
<div class="section" id="manual-splitting-of-data">
<h2>Manual Splitting of Data<a class="headerlink" href="#manual-splitting-of-data" title="Permalink to this headline">¶</a></h2>
<p>The cross-validation in nnU-Net splits on a per-case basis. This may sometimes not be desired, for example because
several training cases may be the same patient (different time steps or annotators). If this is the case, then you need to
manually create a split file. To do this, first let nnU-Net create the default split file. Run one of the network
trainings (any of them works fine for this) and abort after the first epoch. nnU-Net will have created a split file automatically:
<code class="docutils literal notranslate"><span class="pre">preprocessing_output_dir/TaskXX_MY_DATASET/splits_final.pkl</span></code>. This file contains a list (length 5, one entry per fold).
Each entry in the list is a dictionary with keys ‘train’ and ‘val’ pointing to the patientIDs assigned to the sets.
To use your own splits in nnU-Net, you need to edit these entries to what you want them to be and save it back to the
splits_final.pkl file. Use load_pickle and save_pickle from batchgenerators.utilities.file_and_folder_operations for convenience.</p>
</div>
<div class="section" id="do-i-need-to-always-run-all-u-net-configurations">
<h2>Do I need to always run all U-Net configurations?<a class="headerlink" href="#do-i-need-to-always-run-all-u-net-configurations" title="Permalink to this headline">¶</a></h2>
<p>The model training pipeline above is for challenge participations. Depending on your task you may not want to train all
U-Net models and you may also not want to run a cross-validation all the time.
Here are some recommendations about what U-Net model to train:</p>
<ul class="simple">
<li><p>It is safe to say that on average, the 3D U-Net model (3d_fullres) was most robust. If you just want to use nnU-Net because you
need segmentations, I recommend you start with this.</p></li>
<li><p>If you are not happy with the results from the 3D U-Net then you can try the following:</p>
<ul>
<li><p>if your cases are very large so that the patch size of the 3d U-Net only covers a very small fraction of an image then
it is possible that the 3d U-Net cannot capture sufficient contextual information in order to be effective. If this
is the case, you should consider running the 3d U-Net cascade (3d_lowres followed by 3d_cascade_fullres)</p></li>
<li><p>If your data is very anisotropic then a 2D U-Net may actually be a better choice (Promise12, ACDC, Task05_Prostate
from the decathlon are examples for anisotropic data)</p></li>
</ul>
</li>
</ul>
<p>You do not have to run five-fold cross-validation all the time. If you want to test single model performance, use
<em>all</em> for <code class="docutils literal notranslate"><span class="pre">FOLD</span></code> instead of a number. Note that this will then not give you an estimate of your performance on the
training set. You will also no tbe able to automatically identify which ensembling should be used and nnU-Net will
not be able to configure a postprocessing.</p>
<p>CAREFUL: DO NOT use fold=all when you intend to run the cascade! You must run the cross-validation in 3d_lowres so
that you get proper (=not overfitted) low resolution predictions.</p>
</div>
<div class="section" id="sharing-models">
<h2>Sharing Models<a class="headerlink" href="#sharing-models" title="Permalink to this headline">¶</a></h2>
<p>You can share trained models by simply sending the corresponding output folder from <code class="docutils literal notranslate"><span class="pre">RESULTS_FOLDER/nnUNet</span></code> to
whoever you want share them with. The recipient can then use nnU-Net for inference with this model.</p>
</div>
<div class="section" id="can-i-run-nnu-net-on-smaller-gpus">
<h2>Can I run nnU-Net on smaller GPUs?<a class="headerlink" href="#can-i-run-nnu-net-on-smaller-gpus" title="Permalink to this headline">¶</a></h2>
<p>nnU-Net is guaranteed to run on GPUs with 11GB of memory. Many configurations may also run on 8 GB. If you wish to
configure nnU-Net to use a different amount of GPU memory, simply adapt the reference value for the GPU memory estimation
accordingly (with some slack because the whole thing is not an exact science!). For example, in
<a class="reference external" href="nnunet/experiment_planning/experiment_planner_baseline_3DUNet_v21_11GB.py">experiment_planner_baseline_3DUNet_v21_11GB.py</a>
we provide an example that attempts to maximise the usage of GPU memory on 11GB as opposed to the default which leaves
much more headroom). This is simply achieved by this line:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ref</span> <span class="o">=</span> <span class="n">Generic_UNet</span><span class="o">.</span><span class="n">use_this_for_batch_size_computation_3D</span> <span class="o">*</span> <span class="mi">11</span> <span class="o">/</span> <span class="mi">8</span>
</pre></div>
</div>
<p>with 8 being what is currently used (approximately) and 11 being the target. Should you get CUDA out of memory
issues, simply reduce the reference value. You should do this adaptation as part of a separate ExperimentPlanner class.
Please read the instructions <a class="reference internal" href="documentation/extending_nnunet.html"><span class="doc">here</span></a>.</p>
<p>A 32 GB variant is also provided (ExperimentPlanner3D_v21_32GB). Note that increasing the GPU memory target while
remaining on the same GPU will increase the computation time during training and thus the run time substantially!</p>
</div>
<div class="section" id="i-get-the-error-seg-from-prev-stage-missing-when-running-the-cascade">
<h2>I get the error <code class="docutils literal notranslate"><span class="pre">seg</span> <span class="pre">from</span> <span class="pre">prev</span> <span class="pre">stage</span> <span class="pre">missing</span></code> when running the cascade<a class="headerlink" href="#i-get-the-error-seg-from-prev-stage-missing-when-running-the-cascade" title="Permalink to this headline">¶</a></h2>
<p>You need to run all five folds of <code class="docutils literal notranslate"><span class="pre">3d_lowres</span></code>. Segmentations of the previous stage can only be generated from the
validation set, otherwise we would overfit.</p>
</div>
<div class="section" id="why-am-i-getting-runtimeerror-cuda-error-device-side-assert-triggered">
<h2>Why am I getting <code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">CUDA</span> <span class="pre">error:</span> <span class="pre">device-side</span> <span class="pre">assert</span> <span class="pre">triggered</span></code>?<a class="headerlink" href="#why-am-i-getting-runtimeerror-cuda-error-device-side-assert-triggered" title="Permalink to this headline">¶</a></h2>
<p>This error often goes along with something like <code class="docutils literal notranslate"><span class="pre">void</span> <span class="pre">THCudaTensor_scatterFillKernel(TensorInfo&lt;Real,</span> <span class="pre">IndexType&gt;,</span>&#160; <span class="pre">TensorInfo&lt;long,</span> <span class="pre">IndexType&gt;,</span> <span class="pre">Real,</span> <span class="pre">int,</span> <span class="pre">IndexType)</span> <span class="pre">[with</span> <span class="pre">IndexType</span> <span class="pre">=</span> <span class="pre">unsigned</span> <span class="pre">int,</span> <span class="pre">Real</span> <span class="pre">=</span> <span class="pre">float,</span> <span class="pre">Dims</span> <span class="pre">=</span> <span class="pre">-1]:</span>&#160; <span class="pre">block:</span> <span class="pre">[4770,0,0],</span> <span class="pre">thread:</span> <span class="pre">[374,0,0]</span> <span class="pre">Assertion</span> <span class="pre">indexValue</span> <span class="pre">&gt;=</span> <span class="pre">0</span> <span class="pre">&amp;&amp;</span> <span class="pre">indexValue</span> <span class="pre">&lt;</span> <span class="pre">tensor.sizes[dim]</span> <span class="pre">failed.</span></code>.</p>
<p>This means that your dataset contains unexpected values in the segmentations. nnU-Net expects all labels to be
consecutive integers. So if your dataset has 4 classes (background and three foregound labels), then the labels
must be 0, 1, 2, 3 (where 0 must be background!). There cannot be any other values in the ground truth segmentations.</p>
<p>If you run <code class="docutils literal notranslate"><span class="pre">nnUNet_plan_and_preprocess</span></code> with the –verify_dataset_integrity option, this should never happen because
it will check for wrong values in the label images.</p>
</div>
<div class="section" id="why-is-no-3d-lowres-model-created">
<h2>Why is no 3d_lowres model created?<a class="headerlink" href="#why-is-no-3d-lowres-model-created" title="Permalink to this headline">¶</a></h2>
<p>3d_lowres is created only if the patch size in 3d_fullres less than 1/8 of the voxels of the median shape of the data
in 3d_fullres (for example Liver is about 512x512x512 and the patch size is 128x128x128, so that’s 1/64 and thus
3d_lowres is created). You can enforce the creation of 3d_lowres models for smaller datasets by changing the value of
<code class="docutils literal notranslate"><span class="pre">HOW_MUCH_OF_A_PATIENT_MUST_THE_NETWORK_SEE_AT_STAGE0</span></code> (located in experiment_planning.configuration).</p>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Intel® LPOT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>