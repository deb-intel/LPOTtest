

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Step-by-Step &mdash; Intel® Low Precision Optimization Tool  documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> Intel® Low Precision Optimization Tool
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../welcome.html">Introduction to Intel LPOT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/introduction.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/doclist.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../CONTRIBUTING.html">Contributing Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../SECURITY.html">Security Policy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Intel® Low Precision Optimization Tool</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Step-by-Step</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../_sources/examples/tensorflow/image_recognition/slim/README.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="step-by-step">
<h1>Step-by-Step<a class="headerlink" href="#step-by-step" title="Permalink to this headline">¶</a></h1>
<p>This document is used to list steps of reproducing Intel Optimized TensorFlow slim models tuning  result.</p>
<blockquote>
<div><p><strong>Note</strong>:
Slim models are only supported in Intel optimized TF 1.15.x. We use 1.15.2 as an example.</p>
</div></blockquote>
</div>
<div class="section" id="prerequisite">
<h1>Prerequisite<a class="headerlink" href="#prerequisite" title="Permalink to this headline">¶</a></h1>
<div class="section" id="installation">
<h2>1. Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>Recommend python 3.6 or higher version.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip install -r requirements.txt
</pre></div>
</div>
</div>
<div class="section" id="prepare-dataset">
<h2>2. Prepare Dataset<a class="headerlink" href="#prepare-dataset" title="Permalink to this headline">¶</a></h2>
<p>TensorFlow <a class="reference external" href="https://github.com/tensorflow/models">models</a> repo provides <a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/slim#an-automated-script-for-processing-imagenet-data">scripts and instructions</a> to download, process and convert the ImageNet dataset to the TF records format.
We also prepared related scripts in <code class="docutils literal notranslate"><span class="pre">imagenet_prepare</span></code> directory. To download the raw images, the user must create an account with image-net.org. If you have downloaded the raw data and preprocessed the validation data by moving the images into the appropriate sub-directory based on the label (synset) of the image. we can use below command ro convert it to tf records format.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition
<span class="c1"># convert validation subset</span>
bash prepare_dataset.sh --output_dir<span class="o">=</span>./data --raw_dir<span class="o">=</span>/PATH/TO/img_raw/val/ --subset<span class="o">=</span>validation
<span class="c1"># convert train subset</span>
bash prepare_dataset.sh --output_dir<span class="o">=</span>./data --raw_dir<span class="o">=</span>/PATH/TO/img_raw/train/ --subset<span class="o">=</span>train
</pre></div>
</div>
</div>
<div class="section" id="prepare-pre-trained-model">
<h2>3. Prepare pre-trained model<a class="headerlink" href="#prepare-pre-trained-model" title="Permalink to this headline">¶</a></h2>
<p>This tool support slim ckpt file as input for TensorFlow backend, so we can directly download the ckpt model. The demonstrated models are in Google <a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models">models</a>. We will give a example with Inception_v1:</p>
<p>Download the checkpoint file from <a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models">here</a></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz
tar -xvf inception_v1_2016_08_28.tar.gz
</pre></div>
</div>
<blockquote>
<div><p><strong>Note</strong>:
slim model need module tf_slim by default and to run the slim nets, user specific model should define model_func, and arg_scope and register use TFSlimNetsFactory’s register API, there is an example model inception_v4.py and registered in main.py</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">factory</span> <span class="o">=</span> <span class="n">TFSlimNetsFactory</span><span class="p">()</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">299</span><span class="p">,</span> <span class="mi">299</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">factory</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s1">&#39;inception_v4&#39;</span><span class="p">,</span> <span class="n">inception_v4</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">inception_v4_arg_scope</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>tf_slim default supported nets are: [‘alexnet_v2’, ‘overfeat’, ‘vgg_a’, ‘vgg_16’, ‘vgg_19’, ‘inception_v1’, ‘inception_v2’, ‘inception_v3’,’resnet_v1_50’, ‘resnet_v1_101’, ‘resnet_v1_152’, ‘resnet_v1_200’,’resnet_v2_50’, ‘resnet_v2_101’, ‘resnet_v2_152’, ‘resnet_v2_200’]
make sure you input_graph name like the default nets, eg: vgg_16.ckpt will map to nets vgg_16 while vgg16 will throw a not found error.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="run">
<h1>Run<a class="headerlink" href="#run" title="Permalink to this headline">¶</a></h1>
<div class="section" id="tune">
<h2>tune<a class="headerlink" href="#tune" title="Permalink to this headline">¶</a></h2>
<p>./run_tuning.sh –config=model.yaml –input_model=/path/to/input_model.ckpt –output=/path/to/save/lpot_tuned.pb</p>
</div>
<div class="section" id="benchmark">
<h2>benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline">¶</a></h2>
<p>./run_tuning.sh –config=model.yaml –input_model=/path/to/lpot_tuned.pb</p>
<div class="section" id="resnet-v1-50">
<h3>1. resnet_v1_50<a class="headerlink" href="#resnet-v1-50" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition/slim
bash run_tuning.sh --config<span class="o">=</span>resnet_v1_50.yaml <span class="se">\</span>
    --input_model<span class="o">=</span>/PATH/TO/resnet_v1_50.ckpt <span class="se">\</span>
    --output_model<span class="o">=</span>./lpot_resnet_v1_50.pb
</pre></div>
</div>
</div>
<div class="section" id="resnet-v1-101">
<h3>2. resnet_v1_101<a class="headerlink" href="#resnet-v1-101" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition/slim
bash run_tuning.sh --config<span class="o">=</span>../resnet101.yaml <span class="se">\</span>
        --input_model<span class="o">=</span>/PATH/TO/resnet_v1_101.ckpt <span class="se">\</span>
        --output_model<span class="o">=</span>./lpot_resnet_v1_101.pb
</pre></div>
</div>
</div>
<div class="section" id="resnet-v1-152">
<h3>3. resnet_v1_152<a class="headerlink" href="#resnet-v1-152" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition/slim
bash run_tuning.sh --config<span class="o">=</span>resnet_v1_152.yaml <span class="se">\</span>
        --input_model<span class="o">=</span>/PATH/TO/resnet_v1_152.ckpt <span class="se">\</span>
        --output_model<span class="o">=</span>./lpot_resnet_v1_152.pb
</pre></div>
</div>
</div>
<div class="section" id="resnet-v2-50">
<h3>4. resnet_v2_50<a class="headerlink" href="#resnet-v2-50" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition/slim
bash run_tuning.sh --config<span class="o">=</span>../resnet_v2_50.yaml <span class="se">\</span>
        --input_model<span class="o">=</span>/PATH/TO/resnet_v2_50.ckpt <span class="se">\</span>
        --output_model<span class="o">=</span>./lpot_resnet_v2_50.pb
</pre></div>
</div>
</div>
<div class="section" id="resnet-v2-101">
<h3>5. resnet_v2_101<a class="headerlink" href="#resnet-v2-101" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition/slim
bash run_tuning.sh --config<span class="o">=</span>../resnet_v2_101.yaml <span class="se">\</span>
        --input_model<span class="o">=</span>/PATH/TO/resnet_v2_101.ckpt <span class="se">\</span>
        --output_model<span class="o">=</span>./lpot_resnet_v2_101.pb
</pre></div>
</div>
</div>
<div class="section" id="resnet-v2-152">
<h3>6. resnet_v2_152<a class="headerlink" href="#resnet-v2-152" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition/slim
bash run_tuning.sh --config<span class="o">=</span>../resnet_v2_152.yaml <span class="se">\</span>
        --input_model<span class="o">=</span>/PATH/TO/resnet_v2_152.ckpt <span class="se">\</span>
        --output_model<span class="o">=</span>./lpot_resnet_v2_152.pb
</pre></div>
</div>
</div>
<div class="section" id="inception-v1">
<h3>7. inception_v1<a class="headerlink" href="#inception-v1" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition/slim
bash run_tuning.sh --config<span class="o">=</span>../inception_v1.yaml <span class="se">\</span>
        --input_model<span class="o">=</span>/PATH/TO/inception_v1.ckpt <span class="se">\</span>
        --output_model<span class="o">=</span>./lpot_inception_v1.pb
</pre></div>
</div>
</div>
<div class="section" id="inception-v2">
<h3>8. inception_v2<a class="headerlink" href="#inception-v2" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition/slim
bash run_tuning.sh --config<span class="o">=</span>../inception_v2.yaml <span class="se">\</span>
    --input_model<span class="o">=</span>/PATH/TO/inception_v2.ckpt <span class="se">\</span>
    --output_model<span class="o">=</span>./lpot_inception_v2.pb
</pre></div>
</div>
</div>
<div class="section" id="inception-v3">
<h3>9. inception_v3<a class="headerlink" href="#inception-v3" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition/slim
bash run_tuning.sh --config<span class="o">=</span>inception_v3.yaml <span class="se">\</span>
    --input_model<span class="o">=</span>/PATH/TO/inception_v3.ckpt <span class="se">\</span>
    --output_model<span class="o">=</span>./lpot_inception_v3.pb
</pre></div>
</div>
</div>
<div class="section" id="inception-v4">
<h3>10. inception_v4<a class="headerlink" href="#inception-v4" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition/slim
bash run_tuning.sh --config<span class="o">=</span>../inception_v4.yaml <span class="se">\</span>
    --input_model<span class="o">=</span>/PATH/TO/inception_v4.ckpt <span class="se">\</span>
    --output_model<span class="o">=</span>./lpot_inception_v4.pb
</pre></div>
</div>
</div>
<div class="section" id="vgg16">
<h3>11. vgg16<a class="headerlink" href="#vgg16" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition/slim
bash run_tuning.sh --config<span class="o">=</span>../vgg16.yaml <span class="se">\</span>
        --input_model<span class="o">=</span>/PATH/TO/vgg_16.ckpt <span class="se">\</span>
        --output_model<span class="o">=</span>./lpot_vgg_16.pb
</pre></div>
</div>
</div>
<div class="section" id="vgg19">
<h3>12. vgg19<a class="headerlink" href="#vgg19" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> examples/tensorflow/image_recognition/slim
bash run_tuning.sh --config<span class="o">=</span>../vgg19.yaml <span class="se">\</span>
        --input_model<span class="o">=</span>/PATH/TO/vgg_19.ckpt <span class="se">\</span>
        --output_model<span class="o">=</span>./lpot_vgg_19.pb
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="examples-of-enabling-intel-low-precision-optimization-tool-auto-tuning-on-tensorflow-inception-v1">
<h1>Examples of enabling Intel® Low Precision Optimization Tool auto tuning on TensorFlow Inception V1<a class="headerlink" href="#examples-of-enabling-intel-low-precision-optimization-tool-auto-tuning-on-tensorflow-inception-v1" title="Permalink to this headline">¶</a></h1>
<p>This is a tutorial of how to enable a TensorFlow slim model with Intel® Low Precision Optimization Tool.</p>
</div>
<div class="section" id="user-code-analysis">
<h1>User Code Analysis<a class="headerlink" href="#user-code-analysis" title="Permalink to this headline">¶</a></h1>
<p>Intel® Low Precision Optimization Tool supports two usages:</p>
<ol class="simple">
<li><p>User specifies fp32 “model”, yaml configured calibration dataloader in calibration field and evaluation dataloader in evaluation field, metric in tuning.metric field of model-specific yaml config file.</p></li>
</ol>
<blockquote>
<div><p><em>Note</em>:
you should change the model-specific yaml file dataset path to your own dataset path</p>
</div></blockquote>
<ol class="simple">
<li><p>User specifies fp32 “model”, calibration dataset “q_dataloader” and a custom “eval_func” which encapsulates the evaluation dataset and metric by itself.</p></li>
</ol>
<p>As Inception V1 is a typical image recognition model, use Top-K as metric which is built-in supported by Intel® Low Precision Optimization Tool. It’s easy to directly use 1 method that to configure a yaml file.</p>
<div class="section" id="write-yaml-config-file">
<h2>Write Yaml config file<a class="headerlink" href="#write-yaml-config-file" title="Permalink to this headline">¶</a></h2>
<p>In examples directory, there is a template.yaml. We could remove most of items and only keep mandotory item for tuning.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># inceptionv1.yaml</span>

<span class="nt">model</span><span class="p">:</span>                                               <span class="c1"># mandatory. lpot uses this model name and framework name to decide where to save tuning history and deploy yaml.</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">inceptionv1</span>
  <span class="nt">framework</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">tensorflow</span>                              <span class="c1"># mandatory. supported values are tensorflow, pytorch, pytorch_ipex, onnxrt_integer, onnxrt_qlinear or mxnet; allow new framework backend extension.</span>
  <span class="nt">inputs</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">input</span>
  <span class="nt">outputs</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">InceptionV1/Logits/Predictions/Reshape_1</span>

<span class="nt">quantization</span><span class="p">:</span>                                        <span class="c1"># optional. tuning constraints on model-wise for advance user to reduce tuning space.</span>
  <span class="nt">calibration</span><span class="p">:</span>
    <span class="nt">sampling_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">5, 10</span>                             <span class="c1"># optional. default value is 100. used to set how many samples should be used in calibration.</span>
    <span class="nt">dataloader</span><span class="p">:</span>
      <span class="nt">dataset</span><span class="p">:</span>
        <span class="nt">ImageRecord</span><span class="p">:</span>
          <span class="nt">root</span><span class="p">:</span> <span class="nt">/path/to/calibration/dataset         # NOTE</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">modify to calibration dataset location if needed</span>
      <span class="nt">transform</span><span class="p">:</span>
        <span class="nt">ParseDecodeImagenet</span><span class="p">:</span>
        <span class="nt">ResizeCropImagenet</span><span class="p">:</span> 
          <span class="nt">height</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">224</span>
          <span class="nt">width</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">224</span>
          <span class="nt">mean_value</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="nv">123.68</span><span class="p p-Indicator">,</span> <span class="nv">116.78</span><span class="p p-Indicator">,</span> <span class="nv">103.94</span><span class="p p-Indicator">]</span>
  <span class="nt">model_wise</span><span class="p">:</span>                                        <span class="c1"># optional. tuning constraints on model-wise for advance user to reduce tuning space.</span>
    <span class="nt">activation</span><span class="p">:</span>
      <span class="nt">algorithm</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">minmax</span>

<span class="nt">evaluation</span><span class="p">:</span>                                          <span class="c1"># optional. required if user doesn&#39;t provide eval_func in lpot.Quantization.</span>
  <span class="nt">accuracy</span><span class="p">:</span>                                          <span class="c1"># optional. required if user doesn&#39;t provide eval_func in lpot.Quantization.</span>
    <span class="nt">metric</span><span class="p">:</span>
      <span class="nt">topk</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>                                        <span class="c1"># built-in metrics are topk, map, f1, allow user to register new metric.</span>
    <span class="nt">dataloader</span><span class="p">:</span>
      <span class="nt">batch_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">10</span>
      <span class="nt">dataset</span><span class="p">:</span>
        <span class="nt">ImageRecord</span><span class="p">:</span>
          <span class="nt">root</span><span class="p">:</span> <span class="nt">/path/to/evaluation/dataset          # NOTE</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">modify to evaluation dataset location if needed</span>
      <span class="nt">transform</span><span class="p">:</span>
        <span class="nt">ParseDecodeImagenet</span><span class="p">:</span>
        <span class="nt">ResizeCropImagenet</span><span class="p">:</span> 
          <span class="nt">height</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">224</span>
          <span class="nt">width</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">224</span>
          <span class="nt">mean_value</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="nv">123.68</span><span class="p p-Indicator">,</span> <span class="nv">116.78</span><span class="p p-Indicator">,</span> <span class="nv">103.94</span><span class="p p-Indicator">]</span>
  <span class="nt">performance</span><span class="p">:</span>                                       <span class="c1"># optional. used to benchmark performance of passing model.</span>
    <span class="nt">configs</span><span class="p">:</span>
      <span class="nt">cores_per_instance</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">4</span>
      <span class="nt">num_of_instance</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">7</span>
    <span class="nt">dataloader</span><span class="p">:</span>
      <span class="nt">batch_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span> 
      <span class="nt">dataset</span><span class="p">:</span>
        <span class="nt">ImageRecord</span><span class="p">:</span>
          <span class="nt">root</span><span class="p">:</span> <span class="nt">/path/to/evaluation/dataset          # NOTE</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">modify to evaluation dataset location if needed</span>
      <span class="nt">transform</span><span class="p">:</span>
        <span class="nt">ParseDecodeImagenet</span><span class="p">:</span>
        <span class="nt">ResizeCropImagenet</span><span class="p">:</span> 
          <span class="nt">height</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">224</span>
          <span class="nt">width</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">224</span>
          <span class="nt">mean_value</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="nv">123.68</span><span class="p p-Indicator">,</span> <span class="nv">116.78</span><span class="p p-Indicator">,</span> <span class="nv">103.94</span><span class="p p-Indicator">]</span>

<span class="nt">tuning</span><span class="p">:</span>
  <span class="nt">accuracy_criterion</span><span class="p">:</span>
    <span class="nt">relative</span><span class="p">:</span>  <span class="l l-Scalar l-Scalar-Plain">0.01</span>                                  <span class="c1"># optional. default value is relative, other value is absolute. this example allows relative accuracy loss: 1%.</span>
  <span class="nt">exit_policy</span><span class="p">:</span>
    <span class="nt">timeout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>                                       <span class="c1"># optional. tuning timeout (seconds). default value is 0 which means early stop. combine with max_trials field to decide when to exit.</span>
  <span class="nt">random_seed</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">9527</span>                                  <span class="c1"># optional. random seed for deterministic tuning.</span>
</pre></div>
</div>
<p>Here we choose topk built-in metric and set accuracy target as tolerating 0.01 relative accuracy loss of baseline. The default tuning strategy is basic strategy. The timeout 0 means early stop as well as a tuning config meet accuracy target.</p>
</div>
<div class="section" id="prepare">
<h2>prepare<a class="headerlink" href="#prepare" title="Permalink to this headline">¶</a></h2>
<p>There are three preparation steps in here:</p>
<ol class="simple">
<li><p>Prepare environment</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip install intel-tensorflow<span class="o">==</span><span class="m">1</span>.15.2 lpot
</pre></div>
</div>
<ol class="simple">
<li><p>Prepare the ImageNet dataset and pretrainined ckpt file</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz
</pre></div>
</div>
</div>
<div class="section" id="code-update">
<h2>code update<a class="headerlink" href="#code-update" title="Permalink to this headline">¶</a></h2>
<p>This tool support tune and benchmark the model, when in the tune phase, make sure to use get_slim_graph to get the slim graph and thransfer to the tool</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
    <span class="kn">from</span> <span class="nn">lpot.experimental</span> <span class="kn">import</span> <span class="n">Quantization</span>
    <span class="kn">from</span> <span class="nn">lpot.adaptor.tf_utils.util</span> <span class="kn">import</span> <span class="n">get_slim_graph</span>
    <span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
    <span class="n">slim_graph</span> <span class="o">=</span> <span class="n">get_slim_graph</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">input_graph</span><span class="p">,</span> <span class="n">model_func</span><span class="p">,</span> <span class="n">arg_scope</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">q_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="p">(</span><span class="n">slim_graph</span><span class="p">)</span>
    <span class="n">save</span><span class="p">(</span><span class="n">q_model</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">output_graph</span><span class="p">)</span>
</pre></div>
</div>
<p>when in benchmark phase:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
    <span class="kn">from</span> <span class="nn">lpot.experimental</span> <span class="kn">import</span> <span class="n">Benchmark</span>
    <span class="n">evaluator</span> <span class="o">=</span> <span class="n">Benchmark</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">input_graph</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Intel® LPOT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>